{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuIkgmW7Ljl9j76Knx8kLn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wizard339/education/blob/main/nlp_classification_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata"
      ],
      "metadata": {
        "id": "YCP4_0ZlYjfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vDOyVu8lYMve"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS()"
      ],
      "metadata": {
        "id": "KAgK68iDYRJV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "  for dataset in datasets:\n",
        "    for _, text in dataset:\n",
        "      yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), specials=['<UNK>'])\n",
        "vocab.set_default_index(vocab['<UNK>'])"
      ],
      "metadata": {
        "id": "uygeEIydYfu1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer('Text for tokenization test...')\n",
        "indexes = vocab(tokens)\n",
        "\n",
        "tokens, indexes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6maOxcacYPl",
        "outputId": "fcb1db7f-791d-4d16-d351-7f5979ac5691"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['text', 'for', 'tokenization', 'test', '.', '.', '.'],\n",
              " [4003, 11, 0, 287, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.types import Device\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS()\n",
        "train_dataset, test_dataset = to_map_style_dataset(train_dataset), to_map_style_dataset(test_dataset)\n",
        "\n",
        "target_classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "\n",
        "max_words = 25\n",
        "\n",
        "def vectorize_batch(batch):\n",
        "  Y, X = list(zip(*batch))\n",
        "  X = [vocab(tokenizer(text)) for text in X] # tokenize and map tokens to indexes\n",
        "  X = [tokens+([0]*(max_words - len(tokens))) if len(tokens) < max_words else tokens[:max_words] for tokens in X]\n",
        "  return torch.tensor(X, dtype=torch.int32, device='cpu'), torch.tensor(Y, device='cpu') - 1\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, collate_fn=vectorize_batch, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset , batch_size=1024, collate_fn=vectorize_batch)"
      ],
      "metadata": {
        "id": "ys0UGb-udSES"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prompt_toolkit import output\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "EMBED_LEN  = 50\n",
        "HIDDEN_DIM = 75\n",
        "N_LAYERS   = 1\n",
        "\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LSTMClassifier, self).__init__()\n",
        "    self.emb_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=EMBED_LEN)\n",
        "    self.lstm = nn.LSTM(input_size=EMBED_LEN, hidden_size=HIDDEN_DIM, num_layers=N_LAYERS, batch_first=True)\n",
        "    self.fc = nn.Linear(in_features=HIDDEN_DIM, out_features=len(target_classes))\n",
        "\n",
        "  def forward(self, X_batch):\n",
        "    emb = self.emb_layer(X_batch)\n",
        "    output, (h, c) = self.lstm(emb)\n",
        "    return self.fc(output[:, -1])"
      ],
      "metadata": {
        "id": "JnJ7xZHMiAnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_classifier = LSTMClassifier()\n",
        "\n",
        "lstm_classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRwjHcs1lSWb",
        "outputId": "a7703d87-80c9-4a48-a369-b5660844164f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMClassifier(\n",
              "  (emb_layer): Embedding(98635, 50)\n",
              "  (lstm): LSTM(50, 75, batch_first=True)\n",
              "  (fc): Linear(in_features=75, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in lstm_classifier.children():\n",
        "  print(f'Layer: {layer}')\n",
        "  print('Parameters: ')\n",
        "  for param in layer.parameters():\n",
        "    print(param.shape)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CVtUuJvlXbu",
        "outputId": "d0356910-3830-408e-afe9-42e35233712e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: Embedding(98635, 50)\n",
            "Parameters: \n",
            "torch.Size([98635, 50])\n",
            "\n",
            "Layer: LSTM(50, 75, batch_first=True)\n",
            "Parameters: \n",
            "torch.Size([300, 50])\n",
            "torch.Size([300, 75])\n",
            "torch.Size([300])\n",
            "torch.Size([300])\n",
            "\n",
            "Layer: Linear(in_features=75, out_features=4, bias=True)\n",
            "Parameters: \n",
            "torch.Size([4, 75])\n",
            "torch.Size([4])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
        "  with torch.no_grad():\n",
        "    Y_shuffled, Y_preds, losses = [], [], []\n",
        "    for X, Y in val_loader:\n",
        "      preds = model(X)\n",
        "      loss = loss_fn(preds, Y)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      Y_shuffled.append(Y)\n",
        "      Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "    Y_shuffled = torch.cat(Y_shuffled)\n",
        "    Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "    print(f'Valid Loss: {torch.tensor(losses).cpu().mean():.3f}')\n",
        "    print(f'Valid Acc : {accuracy_score(Y_shuffled.cpu().detach().numpy(), Y_preds.cpu().detach().numpy()):.3f}')\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, val_loader, device, epochs=10):\n",
        "\n",
        "  for i in range(1, epochs+1):\n",
        "    losses = []\n",
        "    for X, Y in tqdm(train_loader):\n",
        "      X.to(device)\n",
        "      Y.to(device)\n",
        "      model.to(device)\n",
        "\n",
        "      Y_preds = model(X)\n",
        "\n",
        "      loss = loss_fn(Y_preds, Y)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "    print(f'Train Loss: {torch.tensor(losses).mean():.3f}')\n",
        "    CalcValLossAndAccuracy(model, loss_fn, val_loader)"
      ],
      "metadata": {
        "id": "I_PHcc7xmhda"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXoDW82UqJWJ",
        "outputId": "03e6f891-3d67-4ab1-e8fe-5e765fbc44bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "EPOCHS  = 10\n",
        "LR      = 1e-3\n",
        "OPT     = Adam(lstm_classifier.parameters(), lr=LR)\n",
        "LOSS_FN = nn.CrossEntropyLoss()\n",
        "\n",
        "TrainModel(model=lstm_classifier, loss_fn=LOSS_FN, optimizer=OPT, train_loader=train_loader, val_loader=test_loader, device=device, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKkmMJ-bl8Rw",
        "outputId": "bb1f995b-e041-4517-c644-3e6acd5dd7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:53<00:00,  2.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.290\n",
            "Valid Loss: 0.352\n",
            "Valid Acc : 0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:50<00:00,  2.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.258\n",
            "Valid Loss: 0.343\n",
            "Valid Acc : 0.879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [01:03<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.233\n",
            "Valid Loss: 0.345\n",
            "Valid Acc : 0.882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:43<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.211\n",
            "Valid Loss: 0.342\n",
            "Valid Acc : 0.882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:42<00:00,  2.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.191\n",
            "Valid Loss: 0.338\n",
            "Valid Acc : 0.884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:42<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.171\n",
            "Valid Loss: 0.345\n",
            "Valid Acc : 0.888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:43<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.155\n",
            "Valid Loss: 0.352\n",
            "Valid Acc : 0.887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:42<00:00,  2.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.139\n",
            "Valid Loss: 0.375\n",
            "Valid Acc : 0.885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:42<00:00,  2.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.125\n",
            "Valid Loss: 0.377\n",
            "Valid Acc : 0.884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:45<00:00,  2.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.113\n",
            "Valid Loss: 0.404\n",
            "Valid Acc : 0.885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def MakePredictions(model, loader, device):\n",
        "  Y_shuffled, Y_preds= [], []\n",
        "  for X, Y in loader:\n",
        "    X.to(device)\n",
        "    Y.to(device)\n",
        "    preds = model(X)\n",
        "    Y_preds.append(preds)\n",
        "    Y_shuffled.append(Y)\n",
        "  gc.collect()\n",
        "  Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
        "\n",
        "  return Y_shuffled.cpu().detach().numpy(), F.softmax(Y_preds, dim=-1).argmax(dim=-1)\n",
        "\n",
        "Y_actual, Y_preds = MakePredictions( lstm_classifier, test_loader, device)"
      ],
      "metadata": {
        "id": "K247HM5wssdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(f'Test Accuracy: {accuracy_score(Y_actual, Y_preds)}')\n",
        "print('\\nClassification report: ')\n",
        "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
        "print('\\nConfusion Matrix: ')\n",
        "print(confusion_matrix(Y_actual, Y_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KazXvGMwcny",
        "outputId": "39f61979-cf9f-40fa-c7a0-2e7ea6aaacae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8846052631578948\n",
            "\n",
            "Classification report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.89      0.90      0.89      1900\n",
            "      Sports       0.94      0.95      0.94      1900\n",
            "    Business       0.84      0.86      0.85      1900\n",
            "    Sci/Tech       0.87      0.83      0.85      1900\n",
            "\n",
            "    accuracy                           0.88      7600\n",
            "   macro avg       0.88      0.88      0.88      7600\n",
            "weighted avg       0.88      0.88      0.88      7600\n",
            "\n",
            "\n",
            "Confusion Matrix: \n",
            "[[1717   57   70   56]\n",
            " [  39 1796   30   35]\n",
            " [  89   24 1641  146]\n",
            " [  94   27  210 1569]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using pretrained embeddings from gensim"
      ],
      "metadata": {
        "id": "oZPvufo126Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "\n",
        "w2v = gensim.downloader.load('glove-wiki-gigaword-300')"
      ],
      "metadata": {
        "id": "4SSNQaWm6MUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54535dc0-e60d-48a6-c539-4fd3c0afe3f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_vectors = torch.tensor(w2v.vectors)"
      ],
      "metadata": {
        "id": "dKlqMjMs8yKO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_LEN  = 300\n",
        "HIDDEN_DIM = 75\n",
        "N_LAYERS   = 1\n",
        "\n",
        "\n",
        "class LSTMClassifier2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LSTMClassifier2, self).__init__()\n",
        "    self.emb_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=EMBED_LEN)\n",
        "    self.lstm = nn.LSTM(input_size=EMBED_LEN, hidden_size=HIDDEN_DIM, num_layers=N_LAYERS, batch_first=True)\n",
        "    self.fc = nn.Linear(in_features=HIDDEN_DIM, out_features=len(target_classes))\n",
        "    self.init_embeds()\n",
        "\n",
        "  def init_embeds(self):\n",
        "    weights = w2v_vectors\n",
        "    self.emb_layer = nn.Embedding.from_pretrained(weights)\n",
        "\n",
        "  def forward(self, X_batch):\n",
        "    emb = self.emb_layer(X_batch)\n",
        "    output, (h, c) = self.lstm(emb)\n",
        "    return self.fc(output[:, -1])"
      ],
      "metadata": {
        "id": "h-JLgyXQ0dmU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_classifier_2 = LSTMClassifier2()\n",
        "lstm_classifier_2"
      ],
      "metadata": {
        "id": "BYg21xt17-Ti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514b7fbe-af62-4fa2-a9b7-f30c12288c28"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMClassifier2(\n",
              "  (emb_layer): Embedding(400000, 300)\n",
              "  (lstm): LSTM(300, 75, batch_first=True)\n",
              "  (fc): Linear(in_features=75, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS  = 10\n",
        "LR      = 1e-3\n",
        "OPT     = Adam(lstm_classifier_2.parameters(), lr=LR)\n",
        "LOSS_FN = nn.CrossEntropyLoss()\n",
        "\n",
        "TrainModel(model=lstm_classifier_2, loss_fn=LOSS_FN, optimizer=OPT, train_loader=train_loader, val_loader=test_loader, device=device, epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "AD1VZbnADO3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35bf7bf8-68f8-4bbe-be23-553725d77375"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:55<00:00,  2.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.123\n",
            "Valid Loss: 0.905\n",
            "Valid Acc : 0.635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:55<00:00,  2.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.743\n",
            "Valid Loss: 0.646\n",
            "Valid Acc : 0.760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:56<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.574\n",
            "Valid Loss: 0.551\n",
            "Valid Acc : 0.801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:55<00:00,  2.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.500\n",
            "Valid Loss: 0.548\n",
            "Valid Acc : 0.801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:55<00:00,  2.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.444\n",
            "Valid Loss: 0.501\n",
            "Valid Acc : 0.818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:56<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.411\n",
            "Valid Loss: 0.477\n",
            "Valid Acc : 0.827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:54<00:00,  2.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.380\n",
            "Valid Loss: 0.465\n",
            "Valid Acc : 0.829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:56<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.356\n",
            "Valid Loss: 0.452\n",
            "Valid Acc : 0.840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:55<00:00,  2.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.339\n",
            "Valid Loss: 0.438\n",
            "Valid Acc : 0.841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:55<00:00,  2.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.319\n",
            "Valid Loss: 0.424\n",
            "Valid Acc : 0.848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_actual, Y_preds = MakePredictions( lstm_classifier_2, test_loader, device)"
      ],
      "metadata": {
        "id": "SqHRfHDs0DuO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Test Accuracy: {accuracy_score(Y_actual, Y_preds)}')\n",
        "print('\\nClassification report: ')\n",
        "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
        "print('\\nConfusion Matrix: ')\n",
        "print(confusion_matrix(Y_actual, Y_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cztbMqQH2mVf",
        "outputId": "1a79a073-5f77-406b-97e6-80aa44ca13d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8475\n",
            "\n",
            "Classification report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.83      0.88      0.85      1900\n",
            "      Sports       0.91      0.90      0.91      1900\n",
            "    Business       0.86      0.76      0.81      1900\n",
            "    Sci/Tech       0.80      0.84      0.82      1900\n",
            "\n",
            "    accuracy                           0.85      7600\n",
            "   macro avg       0.85      0.85      0.85      7600\n",
            "weighted avg       0.85      0.85      0.85      7600\n",
            "\n",
            "\n",
            "Confusion Matrix: \n",
            "[[1677   65   65   93]\n",
            " [  91 1718   34   57]\n",
            " [ 156   47 1442  255]\n",
            " [ 102   55  139 1604]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wc1Sd1yC2pGV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}