{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3ZPYzuH-6eY"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQIpV0r2-6ef"
      },
      "source": [
        "\n",
        "# (beta) Dynamic Quantization on an LSTM Word Language Model\n",
        "\n",
        "**Author**: [James Reed](https://github.com/jamesr66a)\n",
        "\n",
        "**Edited by**: [Seth Weidman](https://github.com/SethHWeidman/)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Quantization involves converting the weights and activations of your model from float\n",
        "to int, which can result in smaller model size and faster inference with only a small\n",
        "hit to accuracy.\n",
        "\n",
        "In this tutorial, we will apply the easiest form of quantization -\n",
        "[dynamic quantization](https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic) -\n",
        "to an LSTM-based next word-prediction model, closely following the\n",
        "[word language model](https://github.com/pytorch/examples/tree/master/word_language_model)\n",
        "from the PyTorch examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LverBa7l-6eh"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "from io import open\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb_Vy2T--6ei"
      },
      "source": [
        "## 1. Define the model\n",
        "\n",
        "Here we define the LSTM model architecture, following the\n",
        "[model](https://github.com/pytorch/examples/blob/master/word_language_model/model.py)\n",
        "from the word language model example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jyHIgJ6V-6ej"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7ygyGdG-6ek"
      },
      "source": [
        "## 2. Load in the text data\n",
        "\n",
        "Next, we load the\n",
        "[Wikitext-2 dataset](https://www.google.com/search?q=wikitext+2+data) into a `Corpus`,\n",
        "again following the\n",
        "[preprocessing](https://github.com/pytorch/examples/blob/master/word_language_model/data.py)\n",
        "from the word language model example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aw8Sk5fp-6el",
        "outputId": "e800eea7-6cc0-4427-ba3a-f79a4bd0c9db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-894ffd5adc23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel_data_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data_filepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'wikitext-2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-894ffd5adc23>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-894ffd5adc23>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;34m\"\"\"Tokenizes a text file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Add words to the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids\n",
        "\n",
        "model_data_filepath = 'data/'\n",
        "\n",
        "corpus = Corpus(model_data_filepath + 'wikitext-2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EAD5XCe-6em"
      },
      "source": [
        "## 3. Load the pre-trained model\n",
        "\n",
        "This is a tutorial on dynamic quantization, a quantization technique\n",
        "that is applied after a model has been trained. Therefore, we'll simply load some\n",
        "pre-trained weights into this model architecture; these weights were obtained\n",
        "by training for five epochs using the default settings in the word language model\n",
        "example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJjkkG6e-6em"
      },
      "outputs": [],
      "source": [
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "model = LSTMModel(\n",
        "    ntoken = ntokens,\n",
        "    ninp = 512,\n",
        "    nhid = 256,\n",
        "    nlayers = 5,\n",
        ")\n",
        "\n",
        "model.load_state_dict(\n",
        "    torch.load(\n",
        "        model_data_filepath + 'word_language_model_quantize.pth',\n",
        "        map_location=torch.device('cpu')\n",
        "        )\n",
        "    )\n",
        "\n",
        "model.eval()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyDbw9jg-6en"
      },
      "source": [
        "Now let's generate some text to ensure that the pre-trained model is working\n",
        "properly - similarly to before, we follow\n",
        "[here](https://github.com/pytorch/examples/blob/master/word_language_model/generate.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPTJgfsr-6eo"
      },
      "outputs": [],
      "source": [
        "input_ = torch.randint(ntokens, (1, 1), dtype=torch.long)\n",
        "hidden = model.init_hidden(1)\n",
        "temperature = 1.0\n",
        "num_words = 1000\n",
        "\n",
        "with open(model_data_filepath + 'out.txt', 'w') as outf:\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        for i in range(num_words):\n",
        "            output, hidden = model(input_, hidden)\n",
        "            word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            input_.fill_(word_idx)\n",
        "\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "            outf.write(str(word.encode('utf-8')) + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print('| Generated {}/{} words'.format(i, 1000))\n",
        "\n",
        "with open(model_data_filepath + 'out.txt', 'r') as outf:\n",
        "    all_output = outf.read()\n",
        "    print(all_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpOjfs0O-6eo"
      },
      "source": [
        "It's no GPT-2, but it looks like the model has started to learn the structure of\n",
        "language!\n",
        "\n",
        "We're almost ready to demonstrate dynamic quantization. We just need to define a few more\n",
        "helper functions:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNDMGi4s-6ep"
      },
      "outputs": [],
      "source": [
        "bptt = 25\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "eval_batch_size = 1\n",
        "\n",
        "# create test data set\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    return data.view(bsz, -1).t().contiguous()\n",
        "\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "# Evaluation functions\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target\n",
        "\n",
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "  if isinstance(h, torch.Tensor):\n",
        "      return h.detach()\n",
        "  else:\n",
        "      return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def evaluate(model_, data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model_.eval()\n",
        "    total_loss = 0.\n",
        "    hidden = model_.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model_(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJasi2Ct-6ep"
      },
      "source": [
        "## 4. Test dynamic quantization\n",
        "\n",
        "Finally, we can call ``torch.quantization.quantize_dynamic`` on the model!\n",
        "Specifically,\n",
        "\n",
        "- We specify that we want the ``nn.LSTM`` and ``nn.Linear`` modules in our\n",
        "  model to be quantized\n",
        "- We specify that we want weights to be converted to ``int8`` values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nllMRCee-6eq"
      },
      "outputs": [],
      "source": [
        "import torch.quantization\n",
        "\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "print(quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS38q5PW-6eq"
      },
      "source": [
        "The model looks the same; how has this benefited us? First, we see a\n",
        "significant reduction in model size:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l45XUkTl-6er"
      },
      "outputs": [],
      "source": [
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "print_size_of_model(model)\n",
        "print_size_of_model(quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dalLTiHP-6er"
      },
      "source": [
        "Second, we see faster inference time, with no difference in evaluation loss:\n",
        "\n",
        "Note: we set the number of threads to one for single threaded comparison, since quantized\n",
        "models run single threaded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3qpNrCN-6er"
      },
      "outputs": [],
      "source": [
        "torch.set_num_threads(1)\n",
        "\n",
        "def time_model_evaluation(model, test_data):\n",
        "    s = time.time()\n",
        "    loss = evaluate(model, test_data)\n",
        "    elapsed = time.time() - s\n",
        "    print('''loss: {0:.3f}\\nelapsed time (seconds): {1:.1f}'''.format(loss, elapsed))\n",
        "\n",
        "time_model_evaluation(model, test_data)\n",
        "time_model_evaluation(quantized_model, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9jAnAJM-6es"
      },
      "source": [
        "Running this locally on a MacBook Pro, without quantization, inference takes about 200 seconds,\n",
        "and with quantization it takes just about 100 seconds.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Dynamic quantization can be an easy way to reduce model size while only\n",
        "having a limited effect on accuracy.\n",
        "\n",
        "Thanks for reading! As always, we welcome any feedback, so please create an issue\n",
        "[here](https://github.com/pytorch/pytorch/issues) if you have any.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}