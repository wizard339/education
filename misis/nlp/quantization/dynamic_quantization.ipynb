{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vIN1NeFrvUBr"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf6gzV7-vUBx"
      },
      "source": [
        "\n",
        "# Dynamic Quantization\n",
        "\n",
        "In this recipe you will see how to take advantage of Dynamic\n",
        "Quantization to accelerate inference on an LSTM-style recurrent neural\n",
        "network. This reduces the size of the model weights and speeds up model\n",
        "execution.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "There are a number of trade-offs that can be made when designing neural\n",
        "networks. During model developmenet and training you can alter the\n",
        "number of layers and number of parameters in a recurrent neural network\n",
        "and trade-off accuracy against model size and/or model latency or\n",
        "throughput. Such changes can take lot of time and compute resources\n",
        "because you are iterating over the model training. Quantization gives\n",
        "you a way to make a similar trade off between performance and model\n",
        "accuracy with a known model after training is completed.\n",
        "\n",
        "You can give it a try in a single session and you will certainly reduce\n",
        "your model size significantly and may get a significant latency\n",
        "reduction without losing a lot of accuracy.\n",
        "\n",
        "## What is dynamic quantization?\n",
        "\n",
        "Quantizing a network means converting it to use a reduced precision\n",
        "integer representation for the weights and/or activations. This saves on\n",
        "model size and allows the use of higher throughput math operations on\n",
        "your CPU or GPU.\n",
        "\n",
        "When converting from floating point to integer values you are\n",
        "essentially multiplying the floating point value by some scale factor\n",
        "and rounding the result to a whole number. The various quantization\n",
        "approaches differ in the way they approach determining that scale\n",
        "factor.\n",
        "\n",
        "The key idea with dynamic quantization as described here is that we are\n",
        "going to determine the scale factor for activations dynamically based on\n",
        "the data range observed at runtime. This ensures that the scale factor\n",
        "is \"tuned\" so that as much signal as possible about each observed\n",
        "dataset is preserved.\n",
        "\n",
        "The model parameters on the other hand are known during model conversion\n",
        "and they are converted ahead of time and stored in INT8 form.\n",
        "\n",
        "Arithmetic in the quantized model is done using vectorized INT8\n",
        "instructions. Accumulation is typically done with INT16 or INT32 to\n",
        "avoid overflow. This higher precision value is scaled back to INT8 if\n",
        "the next layer is quantized or converted to FP32 for output.\n",
        "\n",
        "Dynamic quantization is relatively free of tuning parameters which makes\n",
        "it well suited to be added into production pipelines as a standard part\n",
        "of converting LSTM models to deployment.\n",
        "\n",
        "\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Limitations on the approach taken here\n",
        "\n",
        "\n",
        "   This recipe provides a quick introduction to the dynamic quantization\n",
        "   features in PyTorch and the workflow for using it. Our focus is on\n",
        "   explaining the specific functions used to convert the model. We will\n",
        "   make a number of significant simplifications in the interest of brevity\n",
        "   and clarity</p></div>\n",
        "\n",
        "\n",
        "1. You will start with a minimal LSTM network\n",
        "2. You are simply going to initialize the network with a random hidden\n",
        "   state\n",
        "3. You are going to test the network with random inputs\n",
        "4. You are not going to train the network in this tutorial\n",
        "5. You will see that the quantized form of this network is smaller and\n",
        "   runs faster than the floating point network we started with\n",
        "6. You will see that the output values are generally in the same\n",
        "   ballpark as the output of the FP32 network, but we are not\n",
        "   demonstrating here the expected accuracy loss on a real trained\n",
        "   network\n",
        "\n",
        "You will see how dynamic quantization is done and be able to see\n",
        "suggestive reductions in memory use and latency times. Providing a\n",
        "demonstration that the technique can preserve high levels of model\n",
        "accuracy on a trained LSTM is left to a more advanced tutorial. If you\n",
        "want to move right away to that more rigorous treatment please proceed\n",
        "to the [advanced dynamic quantization\n",
        "tutorial](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html)_.\n",
        "\n",
        "## Steps\n",
        "\n",
        "This recipe has 5 steps.\n",
        "\n",
        "1. Set Up - Here you define a very simple LSTM, import modules, and establish\n",
        "   some random input tensors.\n",
        "\n",
        "2. Do the Quantization - Here you instantiate a floating point model and then create quantized\n",
        "   version of it.\n",
        "\n",
        "3. Look at Model Size - Here you show that the model size gets smaller.\n",
        "\n",
        "4. Look at Latency - Here you run the two models and compare model runtime (latency).\n",
        "\n",
        "5. Look at Accuracy - Here you run the two models and compare outputs.\n",
        "\n",
        "\n",
        "### 1: Set Up\n",
        "This is a straightfoward bit of code to set up for the rest of the\n",
        "recipe.\n",
        "\n",
        "The unique module we are importing here is torch.quantization which\n",
        "includes PyTorch's quantized operators and conversion functions. We also\n",
        "define a very simple LSTM model and set up some inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jWkfhOVqvUB0"
      },
      "outputs": [],
      "source": [
        "# import the modules used here in this recipe\n",
        "import torch\n",
        "import torch.quantization\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "\n",
        "# define a very, very simple LSTM for demonstration purposes\n",
        "# in this case, we are wrapping nn.LSTM, one layer, no pre or post processing\n",
        "# inspired by\n",
        "# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html, by Robert Guthrie\n",
        "# and https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\n",
        "class lstm_for_demonstration(nn.Module):\n",
        "  \"\"\"Elementary Long Short Term Memory style model which simply wraps nn.LSTM\n",
        "     Not to be used for anything other than demonstration.\n",
        "  \"\"\"\n",
        "  def __init__(self,in_dim,out_dim,depth):\n",
        "     super(lstm_for_demonstration,self).__init__()\n",
        "     self.lstm = nn.LSTM(in_dim,out_dim,depth)\n",
        "\n",
        "  def forward(self,inputs,hidden):\n",
        "     out,hidden = self.lstm(inputs,hidden)\n",
        "     return out, hidden\n",
        "\n",
        "\n",
        "torch.manual_seed(29592)  # set the seed for reproducibility\n",
        "\n",
        "#shape parameters\n",
        "model_dimension=8\n",
        "sequence_length=20\n",
        "batch_size=1\n",
        "lstm_depth=1\n",
        "\n",
        "# random data for input\n",
        "inputs = torch.randn(sequence_length,batch_size,model_dimension)\n",
        "# hidden is actually is a tuple of the initial hidden state and the initial cell state\n",
        "hidden = (torch.randn(lstm_depth,batch_size,model_dimension), torch.randn(lstm_depth,batch_size,model_dimension))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI_mb6ivvUB1"
      },
      "source": [
        "### 2: Do the Quantization\n",
        "\n",
        "Now we get to the fun part. First we create an instance of the model\n",
        "called float\\_lstm then we are going to quantize it. We're going to use\n",
        "the\n",
        "\n",
        "::\n",
        "\n",
        "    torch.quantization.quantize_dynamic()\n",
        "\n",
        "function here ([see\n",
        "documentation](https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic)_)\n",
        "which takes the model, then a list of the submodules which we want to\n",
        "have quantized if they appear, then the datatype we are targeting. This\n",
        "function returns a quantized version of the original model as a new\n",
        "module.\n",
        "\n",
        "That's all it takes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9P0gcMrZvUB2",
        "outputId": "43a54ae2-194c-446d-b866-441ee6ba6fdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the floating point version of this module:\n",
            "lstm_for_demonstration(\n",
            "  (lstm): LSTM(8, 8)\n",
            ")\n",
            "\n",
            "and now the quantized version:\n",
            "lstm_for_demonstration(\n",
            "  (lstm): DynamicQuantizedLSTM(8, 8)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# here is our floating point instance\n",
        "float_lstm = lstm_for_demonstration(model_dimension, model_dimension,lstm_depth)\n",
        "\n",
        "# this is the call that does the work\n",
        "quantized_lstm = torch.quantization.quantize_dynamic(\n",
        "    float_lstm, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# show the changes that were made\n",
        "print('Here is the floating point version of this module:')\n",
        "print(float_lstm)\n",
        "print('')\n",
        "print('and now the quantized version:')\n",
        "print(quantized_lstm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzePxLiFvUB3"
      },
      "source": [
        "### 3. Look at Model Size\n",
        "Ok, so we've quantized the model. What does that get us? Well the first\n",
        "benefit is that we've replaced the FP32 model parameters with INT8\n",
        "values (and some recorded scale factors). This means about 75% less data\n",
        "to store and move around. With the default values the reduction shown\n",
        "below will be less than 75% but if you increase the model size above\n",
        "(for example you can set model dimension to something like 80) this will\n",
        "converge towards 4x smaller as the stored model size dominated more and\n",
        "more by the parameter values.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4b5l39dEvUB3",
        "outputId": "93a3f827-7859-4c13-e7b1-c834acb70c64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  fp32  \t Size (KB): 3.661\n",
            "model:  int8  \t Size (KB): 2.573\n",
            "1.42 times smaller\n"
          ]
        }
      ],
      "source": [
        "def print_size_of_model(model, label=\"\"):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size=os.path.getsize(\"temp.p\")\n",
        "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
        "    os.remove('temp.p')\n",
        "    return size\n",
        "\n",
        "# compare the sizes\n",
        "f=print_size_of_model(float_lstm,\"fp32\")\n",
        "q=print_size_of_model(quantized_lstm,\"int8\")\n",
        "print(\"{0:.2f} times smaller\".format(f/q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJHRH18CvUB4"
      },
      "source": [
        "### 4. Look at Latency\n",
        "The second benefit is that the quantized model will typically run\n",
        "faster. This is due to a combinations of effects including at least:\n",
        "\n",
        "1. Less time spent moving parameter data in\n",
        "2. Faster INT8 operations\n",
        "\n",
        "As you will see the quantized version of this super-simple network runs\n",
        "faster. This will generally be true of more complex networks but as they\n",
        "say \"your milage may vary\" depending on a number of factors including\n",
        "the structure of the model and the hardware you are running on.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6k9jQSxkvUB5",
        "outputId": "c6f51645-71e7-4abb-a9c7-3457933906d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Floating point FP32\n",
            "1.42 ms ± 52.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "Quantized INT8\n",
            "924 µs ± 279 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "# compare the performance\n",
        "print(\"Floating point FP32\")\n",
        "%timeit float_lstm.forward(inputs, hidden)\n",
        "\n",
        "print(\"Quantized INT8\")\n",
        "%timeit quantized_lstm.forward(inputs,hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgIZKCfjvUB6"
      },
      "source": [
        "### 5: Look at Accuracy\n",
        "We are not going to do a careful look at accuracy here because we are\n",
        "working with a randomly initialized network rather than a properly\n",
        "trained one. However, I think it is worth quickly showing that the\n",
        "quantized network does produce output tensors that are \"in the same\n",
        "ballpark\" as the original one.\n",
        "\n",
        "For a more detailed analysis please see the more advanced tutorials\n",
        "referenced at the end of this recipe.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rsNP8kwzvUB7",
        "outputId": "8d987686-eeb0-4db1-ca43-a2a5c1ccc0c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean absolute value of output tensor values in the FP32 model is 0.12887 \n",
            "mean absolute value of output tensor values in the INT8 model is 0.12912\n",
            "mean absolute value of the difference between the output tensors is 0.00156 or 1.21 percent\n"
          ]
        }
      ],
      "source": [
        "# run the float model\n",
        "out1, hidden1 = float_lstm(inputs, hidden)\n",
        "mag1 = torch.mean(abs(out1)).item()\n",
        "print('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n",
        "\n",
        "# run the quantized model\n",
        "out2, hidden2 = quantized_lstm(inputs, hidden)\n",
        "mag2 = torch.mean(abs(out2)).item()\n",
        "print('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n",
        "\n",
        "# compare them\n",
        "mag3 = torch.mean(abs(out1-out2)).item()\n",
        "print('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKDW-yYVvUB7"
      },
      "source": [
        "## Learn More\n",
        "We've explained what dynamic quantization is, what benefits it brings,\n",
        "and you have used the ``torch.quantization.quantize_dynamic()`` function\n",
        "to quickly quantize a simple LSTM model.\n",
        "\n",
        "This was a fast and high level treatment of this material; for more\n",
        "detail please continue learning with [(beta) Dynamic Quantization on an LSTM Word Language Model Tutorial](https://pytorch.org/tutorials/advanced/dynamic\\_quantization\\_tutorial.html).\n",
        "\n",
        "\n",
        "# Additional Resources\n",
        "Documentation\n",
        "~~~~~~~~~~~~~~\n",
        "\n",
        "[Quantization API Documentaion](https://pytorch.org/docs/stable/quantization.html)\n",
        "\n",
        "### Tutorials\n",
        "\n",
        "[(beta) Dynamic Quantization on BERT](https://pytorch.org/tutorials/intermediate/dynamic\\_quantization\\_bert\\_tutorial.html)\n",
        "\n",
        "[(beta) Dynamic Quantization on an LSTM Word Language Model](https://pytorch.org/tutorials/advanced/dynamic\\_quantization\\_tutorial.html)\n",
        "\n",
        "### Blogs\n",
        "[Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}