{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_zLNARu9cYDv",
        "yKD4GgGpfybD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wizard339/education/blob/main/misis/nlp/token_classification/seminar_3_10_10_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFIUUYWtVMRB"
      },
      "source": [
        "# Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C6hPcSsPF42"
      },
      "source": [
        "In this lecture we will get insight into very popular NLP task - Named Entity Recognition.<br>Our goal is to:\n",
        "- build a good baseline solution\n",
        "- modify the data markup\n",
        "- learn how to solve this problem using neural network methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iilmDaCFVOXX"
      },
      "source": [
        "In first part we will explore how to get fast solution of this task, how to exlore metrics and how to convert labeling.<br>\n",
        "In the second part we will look how we can solve this task by using different architectures and measure them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsLFL7s0VRsK"
      },
      "source": [
        "What we will learn:\n",
        "- non neural approaches for NER-task;\n",
        "- measure quality of model for NER-task;\n",
        "- different markup for NER-task;\n",
        "- data preparation for neural network solution of NER;\n",
        "- using different neural approaches for NER;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIKjsBqIVU2x"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrym_4yiVWok"
      },
      "source": [
        "## Solving NER task without Neural netowrks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdYixEjkRfrC"
      },
      "source": [
        "!pip install datasets > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI6yaeHZRjbc"
      },
      "source": [
        "import pytest\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import LongTensor, FloatTensor\n",
        "from torch.nn import functional as F\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from torch.utils.data import Dataset\n",
        "from torch.optim import Adam\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilQEoIuGW6YB"
      },
      "source": [
        "### look at the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoASj4VoRrqN"
      },
      "source": [
        "For this task we will use common NER-dataset which is always included in all benchmarks, when scientists measure quality of SOTA solutions for NER.<br>\n",
        "The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzSRKAzLRpT_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "22785f056bfd493c81b86e25fb42a305",
            "a141f5e9cb624f8187255e964aeefa5d",
            "3e165437ecb64e62949d7ee2490b4d19",
            "547594ab61084d28988a45a942c5ef29",
            "a4fe2e7f53154074adfddf56879bcddd",
            "1a2167a91a824c7ab044210d71aed6ba",
            "e7df4a7f1e5f4968b29750b2cd87b062",
            "62c0b8c7130240c99115a99ee89b2091",
            "8b039a7a5669448bac3b1e39c944ff07",
            "5374fb540e354b5793ca530b70f77e94",
            "9ad2f1c1702a44ac93872e1107b534fd"
          ]
        },
        "outputId": "fc830ea9-7e84-4107-ccf2-45e2a6add47b"
      },
      "source": [
        "dataset_base = load_dataset(\"conll2003\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22785f056bfd493c81b86e25fb42a305",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.60k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a141f5e9cb624f8187255e964aeefa5d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.78k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset conll2003/conll2003 (download: 4.63 MiB, generated: 9.78 MiB, post-processed: Unknown size, total: 14.41 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e165437ecb64e62949d7ee2490b4d19",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "547594ab61084d28988a45a942c5ef29",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/650k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4fe2e7f53154074adfddf56879bcddd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/163k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a2167a91a824c7ab044210d71aed6ba",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/146k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7df4a7f1e5f4968b29750b2cd87b062",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62c0b8c7130240c99115a99ee89b2091",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b039a7a5669448bac3b1e39c944ff07",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5374fb540e354b5793ca530b70f77e94",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ad2f1c1702a44ac93872e1107b534fd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju3RL22uR_Tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb59a5a-e4da-4d4e-ef6e-7aa510c64090"
      },
      "source": [
        "dataset_base['train']['ner_tags'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 0, 7, 0, 0, 0, 7, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SejZA2vpSmUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b070c96f-08bd-4365-b7b8-ae2f80c8d3d2"
      },
      "source": [
        "import json\n",
        "mapping_ = {v: k for k, v in dataset_base[\"train\"].features[\"ner_tags\"].feature._str2int.items()}\n",
        "\n",
        "with open('mapping.json', 'w') as f:\n",
        "  json.dump(mapping_, f)\n",
        "mapping_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-PER',\n",
              " 2: 'I-PER',\n",
              " 3: 'B-ORG',\n",
              " 4: 'I-ORG',\n",
              " 5: 'B-LOC',\n",
              " 6: 'I-LOC',\n",
              " 7: 'B-MISC',\n",
              " 8: 'I-MISC'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3dkmzC6TGh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39656e50-4ffc-4280-bde4-3556a7e59a17"
      },
      "source": [
        "for i in range(10):\n",
        "  print(i + 1, ' '.join(dataset_base[\"train\"]['tokens'][i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 EU rejects German call to boycott British lamb .\n",
            "2 Peter Blackburn\n",
            "3 BRUSSELS 1996-08-22\n",
            "4 The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep .\n",
            "5 Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\n",
            "6 \" We do n't support any such recommendation because we do n't see any grounds for it , \" the Commission 's chief spokesman Nikolaus van der Pas told a news briefing .\n",
            "7 He said further scientific study was required and if it was found that action was needed it should be taken by the European Union .\n",
            "8 He said a proposal last month by EU Farm Commissioner Franz Fischler to ban sheep brains , spleens and spinal cords from the human and animal food chains was a highly specific and precautionary move to protect human health .\n",
            "9 Fischler proposed EU-wide measures after reports from Britain and France that under laboratory conditions sheep could contract Bovine Spongiform Encephalopathy ( BSE ) -- mad cow disease .\n",
            "10 But Fischler agreed to review his proposal after the EU 's standing veterinary committee , mational animal health officials , questioned if such action was justified as there was only a slight risk to human health .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f1pzbaOJjB2"
      },
      "source": [
        "#### Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bujckdrmJl7d"
      },
      "source": [
        "Count occurence of each entity. Print number of occurences for each entity. Result must be a dictinary, where keys are entities from `dataset_base[\"train\"]['ner_tags']` and values are total number of occurencies for each key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9rep5PhQ2z-"
      },
      "source": [
        "# %time\n",
        "# from collections import defaultdict\n",
        "# counter = defaultdict(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9McyL86cU5dV"
      },
      "source": [
        "# dataset_base['train']['tokens'][:3]\n",
        "# dataset_base['train']['ner_tags'][:3]\n",
        "# dataset_base['train'].features['ner_tags']\n",
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmpBc17GTro1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a28716-a40c-4739-e1b7-9a3c8634f097"
      },
      "source": [
        "%time\n",
        "# YOUR CODE HERE\n",
        "\n",
        "counter = defaultdict(int)\n",
        "for tags in dataset_base[\"train\"]['ner_tags']:\n",
        "  for tag in tags:\n",
        "    counter[mapping_[tag]] += 1\n",
        "counter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 8.34 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'B-LOC': 7140,\n",
              "             'B-MISC': 3438,\n",
              "             'B-ORG': 6321,\n",
              "             'B-PER': 6600,\n",
              "             'I-LOC': 1157,\n",
              "             'I-MISC': 1155,\n",
              "             'I-ORG': 3704,\n",
              "             'I-PER': 4528,\n",
              "             'O': 169578})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSwUL6qYnIjU"
      },
      "source": [
        "assert len(counter) == 9\n",
        "assert counter['O'] > 169000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOC2dHK0X9SL"
      },
      "source": [
        "As you see, we have dominating number of class `O`. Our main goal is to make such model, that will not overfit to predict always `O` token.<br>\n",
        "What metrics are more appropriate to measure quality of models for NER?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA5g7XGEJTya"
      },
      "source": [
        "### Sklearn-crf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pwR8trFYXmr"
      },
      "source": [
        "Now I'd like to introduce you great library, that can provide light and easy implementation for solving NER-task. It's name is `sklearn-crf`. It has familiar interface to basic sklearn, but is based on very powerful tool for NER-task - CRF(Conditional Random Field). <br>\n",
        "CRF is nowdays the de facto standard for solving the NLP problem. Even in the most modern SOTA neural networks approaches, a CRF layer can now often be seen as an output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uco_i3lZgUd"
      },
      "source": [
        "!pip install sklearn_crfsuite > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcTxcqd6Zhb9"
      },
      "source": [
        "import sklearn\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1-YPeBkZqJa"
      },
      "source": [
        "As all sklearn-like libraries we need to get pandas.DataFrame as an input for this model. Let's create it.<br>\n",
        "In our DataFrame we will make each word, entity and sentence_id on each row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htp-v1JRZmm-"
      },
      "source": [
        "df = pd.DataFrame({'sent_id': [i for j in [[i] * len(s['tokens']) for i, s in enumerate(dataset_base['train'])] for i in j],\n",
        "                   'data': [i for j in dataset_base['train'] for i in j['tokens']],\n",
        "                   'entities': [mapping_[i] for j in dataset_base['train'] for i in j['ner_tags']]})\n",
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oAym1Z_WjM8"
      },
      "source": [
        "# df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeY96hlsZ-YT"
      },
      "source": [
        "Now we have dataframe, where only 3 columns exsists:\n",
        " - sentense_id - which mark each word belonging to each sentence\n",
        " - data contains words on each row\n",
        " - entities marks which entity does each word refer to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WImNIQKwbHYs"
      },
      "source": [
        "We also need a class, that will process each sentence and aggregate words and entities in it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAcrp66Qa7mY"
      },
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s['data'].values.tolist(), \n",
        "                                                     s['entities'].values.tolist())]\n",
        "        self.grouped = self.data.groupby('sent_id').apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "        \n",
        "    def get_next(self):\n",
        "        try: \n",
        "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s \n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa-NFSnKbDEd"
      },
      "source": [
        "getter = SentenceGetter(df)\n",
        "sentences = getter.sentences\n",
        "sentences[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQsEoGBsbXwl"
      },
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "    \n",
        "    features = {\n",
        "        'bias': 1.0, \n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit()\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        features.update({\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper()\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        features.update({\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper()\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, label in sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txY55dbQb1DH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e42f76f-683b-4e40-a8ca-bc950f3a4af5"
      },
      "source": [
        "X = [sent2features(s) for s in sentences]\n",
        "y = [sent2labels(s) for s in sentences]\n",
        "len(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14041"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMlf5JJrb7WW"
      },
      "source": [
        "X_train = X[:10000]\n",
        "X_test = X[10000:]\n",
        "y_train = y[:10000]\n",
        "y_test = y[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOjIhfYcb-CK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e6554e-f700-4ca5-ec9f-1235ac1d049e"
      },
      "source": [
        "%%time\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True,\n",
        "    verbose=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading training data to CRFsuite: 100%|██████████| 10000/10000 [00:01<00:00, 7069.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature generation\n",
            "type: CRF1d\n",
            "feature.minfreq: 0.000000\n",
            "feature.possible_states: 0\n",
            "feature.possible_transitions: 1\n",
            "0....1....2....3....4....5....6....7....8....9....10\n",
            "Number of features: 68239\n",
            "Seconds required: 0.279\n",
            "\n",
            "L-BFGS optimization\n",
            "c1: 0.100000\n",
            "c2: 0.100000\n",
            "num_memories: 6\n",
            "max_iterations: 100\n",
            "epsilon: 0.000010\n",
            "stop: 10\n",
            "delta: 0.000010\n",
            "linesearch: MoreThuente\n",
            "linesearch.max_iterations: 20\n",
            "\n",
            "Iter 1   time=0.30  loss=172342.13 active=68004 feature_norm=1.00\n",
            "Iter 2   time=0.32  loss=130286.02 active=66816 feature_norm=3.00\n",
            "Iter 3   time=0.15  loss=109044.75 active=65678 feature_norm=2.57\n",
            "Iter 4   time=0.31  loss=94855.47 active=66280 feature_norm=2.23\n",
            "Iter 5   time=0.16  loss=86509.90 active=67032 feature_norm=2.58\n",
            "Iter 6   time=0.17  loss=76660.45 active=67574 feature_norm=3.25\n",
            "Iter 7   time=0.17  loss=60714.33 active=67292 feature_norm=5.33\n",
            "Iter 8   time=0.16  loss=53334.59 active=66153 feature_norm=6.21\n",
            "Iter 9   time=0.15  loss=47394.52 active=55027 feature_norm=8.14\n",
            "Iter 10  time=0.16  loss=42682.31 active=54600 feature_norm=9.30\n",
            "Iter 11  time=0.15  loss=39755.34 active=53899 feature_norm=10.95\n",
            "Iter 12  time=0.16  loss=36717.45 active=52733 feature_norm=12.41\n",
            "Iter 13  time=0.16  loss=33081.93 active=50066 feature_norm=14.77\n",
            "Iter 14  time=0.16  loss=30104.84 active=46244 feature_norm=18.18\n",
            "Iter 15  time=0.16  loss=27433.58 active=45601 feature_norm=20.71\n",
            "Iter 16  time=0.16  loss=24732.23 active=44314 feature_norm=24.06\n",
            "Iter 17  time=0.15  loss=21878.18 active=42055 feature_norm=31.81\n",
            "Iter 18  time=0.16  loss=19482.24 active=41902 feature_norm=34.95\n",
            "Iter 19  time=0.16  loss=18241.56 active=41277 feature_norm=38.50\n",
            "Iter 20  time=0.17  loss=16918.03 active=39504 feature_norm=46.71\n",
            "Iter 21  time=0.16  loss=15124.00 active=38047 feature_norm=50.30\n",
            "Iter 22  time=0.16  loss=13869.13 active=36713 feature_norm=55.72\n",
            "Iter 23  time=0.17  loss=12422.34 active=36401 feature_norm=62.86\n",
            "Iter 24  time=0.16  loss=11311.81 active=36159 feature_norm=69.20\n",
            "Iter 25  time=0.17  loss=10170.09 active=35071 feature_norm=77.21\n",
            "Iter 26  time=0.16  loss=9228.19  active=33682 feature_norm=84.67\n",
            "Iter 27  time=0.15  loss=8591.07  active=32683 feature_norm=93.57\n",
            "Iter 28  time=0.16  loss=8034.03  active=32892 feature_norm=95.73\n",
            "Iter 29  time=0.17  loss=7647.08  active=32333 feature_norm=99.65\n",
            "Iter 30  time=0.16  loss=7145.39  active=31917 feature_norm=104.81\n",
            "Iter 31  time=0.16  loss=6675.89  active=30843 feature_norm=109.60\n",
            "Iter 32  time=0.62  loss=6607.35  active=30675 feature_norm=111.13\n",
            "Iter 33  time=0.16  loss=6301.12  active=30269 feature_norm=114.25\n",
            "Iter 34  time=0.16  loss=6052.80  active=29838 feature_norm=117.54\n",
            "Iter 35  time=0.16  loss=5978.07  active=27391 feature_norm=122.71\n",
            "Iter 36  time=0.16  loss=5753.62  active=27561 feature_norm=123.35\n",
            "Iter 37  time=0.16  loss=5706.37  active=27482 feature_norm=123.73\n",
            "Iter 38  time=0.16  loss=5607.37  active=27088 feature_norm=124.85\n",
            "Iter 39  time=0.17  loss=5581.04  active=26171 feature_norm=128.29\n",
            "Iter 40  time=0.17  loss=5462.59  active=26458 feature_norm=127.65\n",
            "Iter 41  time=0.16  loss=5441.88  active=26368 feature_norm=127.57\n",
            "Iter 42  time=0.16  loss=5379.43  active=26021 feature_norm=127.71\n",
            "Iter 43  time=0.32  loss=5355.37  active=25550 feature_norm=127.96\n",
            "Iter 44  time=0.16  loss=5318.09  active=25278 feature_norm=128.29\n",
            "Iter 45  time=0.16  loss=5288.16  active=24957 feature_norm=129.10\n",
            "Iter 46  time=0.16  loss=5258.55  active=24704 feature_norm=129.87\n",
            "Iter 47  time=0.16  loss=5231.55  active=24493 feature_norm=130.46\n",
            "Iter 48  time=0.16  loss=5211.98  active=24147 feature_norm=131.00\n",
            "Iter 49  time=0.16  loss=5194.10  active=23618 feature_norm=131.39\n",
            "Iter 50  time=0.18  loss=5180.34  active=23402 feature_norm=131.54\n",
            "Iter 51  time=0.16  loss=5166.24  active=23161 feature_norm=131.60\n",
            "Iter 52  time=0.17  loss=5154.91  active=22734 feature_norm=131.72\n",
            "Iter 53  time=0.15  loss=5142.58  active=22660 feature_norm=131.87\n",
            "Iter 54  time=0.16  loss=5135.56  active=22535 feature_norm=131.93\n",
            "Iter 55  time=0.16  loss=5125.66  active=22250 feature_norm=132.08\n",
            "Iter 56  time=0.15  loss=5117.51  active=22061 feature_norm=132.14\n",
            "Iter 57  time=0.16  loss=5110.43  active=21950 feature_norm=132.21\n",
            "Iter 58  time=0.17  loss=5103.38  active=21762 feature_norm=132.22\n",
            "Iter 59  time=0.17  loss=5098.08  active=21596 feature_norm=132.30\n",
            "Iter 60  time=0.17  loss=5093.11  active=21507 feature_norm=132.35\n",
            "Iter 61  time=0.16  loss=5089.25  active=21477 feature_norm=132.36\n",
            "Iter 62  time=0.15  loss=5085.26  active=21378 feature_norm=132.42\n",
            "Iter 63  time=0.16  loss=5082.43  active=21285 feature_norm=132.46\n",
            "Iter 64  time=0.16  loss=5078.30  active=21198 feature_norm=132.52\n",
            "Iter 65  time=0.16  loss=5075.87  active=21142 feature_norm=132.56\n",
            "Iter 66  time=0.16  loss=5073.28  active=21099 feature_norm=132.61\n",
            "Iter 67  time=0.16  loss=5071.37  active=21025 feature_norm=132.63\n",
            "Iter 68  time=0.15  loss=5068.91  active=20972 feature_norm=132.68\n",
            "Iter 69  time=0.15  loss=5067.04  active=20926 feature_norm=132.70\n",
            "Iter 70  time=0.15  loss=5065.08  active=20857 feature_norm=132.74\n",
            "Iter 71  time=0.16  loss=5063.42  active=20792 feature_norm=132.76\n",
            "Iter 72  time=0.16  loss=5061.68  active=20753 feature_norm=132.79\n",
            "Iter 73  time=0.16  loss=5060.30  active=20714 feature_norm=132.80\n",
            "Iter 74  time=0.16  loss=5058.74  active=20678 feature_norm=132.83\n",
            "Iter 75  time=0.16  loss=5057.55  active=20643 feature_norm=132.84\n",
            "Iter 76  time=0.17  loss=5056.04  active=20621 feature_norm=132.86\n",
            "Iter 77  time=0.16  loss=5055.04  active=20604 feature_norm=132.87\n",
            "Iter 78  time=0.16  loss=5053.76  active=20586 feature_norm=132.89\n",
            "Iter 79  time=0.16  loss=5052.81  active=20561 feature_norm=132.90\n",
            "Iter 80  time=0.16  loss=5051.60  active=20536 feature_norm=132.91\n",
            "Iter 81  time=0.16  loss=5050.72  active=20523 feature_norm=132.93\n",
            "Iter 82  time=0.15  loss=5049.72  active=20512 feature_norm=132.93\n",
            "Iter 83  time=0.16  loss=5048.89  active=20480 feature_norm=132.95\n",
            "Iter 84  time=0.16  loss=5047.97  active=20441 feature_norm=132.96\n",
            "Iter 85  time=0.16  loss=5047.16  active=20431 feature_norm=132.97\n",
            "Iter 86  time=0.16  loss=5046.36  active=20415 feature_norm=132.97\n",
            "Iter 87  time=0.15  loss=5045.61  active=20400 feature_norm=132.98\n",
            "Iter 88  time=0.15  loss=5044.89  active=20371 feature_norm=132.99\n",
            "Iter 89  time=0.16  loss=5044.10  active=20356 feature_norm=132.99\n",
            "Iter 90  time=0.16  loss=5043.41  active=20330 feature_norm=133.00\n",
            "Iter 91  time=0.16  loss=5042.67  active=20328 feature_norm=133.00\n",
            "Iter 92  time=0.16  loss=5042.11  active=20313 feature_norm=133.00\n",
            "Iter 93  time=0.16  loss=5041.35  active=20300 feature_norm=133.01\n",
            "Iter 94  time=0.16  loss=5040.82  active=20278 feature_norm=133.01\n",
            "Iter 95  time=0.16  loss=5040.05  active=20259 feature_norm=133.01\n",
            "Iter 96  time=0.17  loss=5039.51  active=20251 feature_norm=133.01\n",
            "Iter 97  time=0.16  loss=5038.83  active=20226 feature_norm=133.00\n",
            "Iter 98  time=0.16  loss=5038.17  active=20200 feature_norm=132.99\n",
            "Iter 99  time=0.16  loss=5037.61  active=20191 feature_norm=132.99\n",
            "Iter 100 time=0.16  loss=5036.88  active=20185 feature_norm=132.98\n",
            "L-BFGS terminated with the maximum number of iterations\n",
            "Total seconds required for training: 17.080\n",
            "\n",
            "Storing the model\n",
            "Number of active features: 20185 (68239)\n",
            "Number of active attributes: 13596 (55010)\n",
            "Number of active labels: 9 (9)\n",
            "Writing labels\n",
            "Writing attributes\n",
            "Writing feature references for transitions\n",
            "Writing feature references for attributes\n",
            "Seconds required: 0.015\n",
            "\n",
            "CPU times: user 18.7 s, sys: 149 ms, total: 18.8 s\n",
            "Wall time: 18.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h064girfcGdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71edbf42-5fcd-4eac-d3e8-315a11372e94"
      },
      "source": [
        "all_entities = sorted(df.entities.unique().tolist())\n",
        "y_pred = crf.predict(X_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=all_entities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9670912566610134"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOg9TIVOKOKD"
      },
      "source": [
        "#### Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF4g_dZmKP0p"
      },
      "source": [
        "Print classification report for all useful tokens (exluding token `O`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myDSv6zPcM4R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "332ae7fc-2281-48cf-f58f-d0fa28f72eac"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test, y_pred, labels=all_entities[:-1]))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-934bc375ef9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_entities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFR46OtaJzsj"
      },
      "source": [
        "#### Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVnO-hahJ1di"
      },
      "source": [
        "Make some additional features to reach at least 0.82 weighted f1-score on detection all useful tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zLNARu9cYDv"
      },
      "source": [
        "##### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXaPZQVMcU7V"
      },
      "source": [
        "# 1. You can check for lower() each word\n",
        "# 2. You can add more words to features, for example last 3 words words[-3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXHti7ticZYU"
      },
      "source": [
        "##### continue work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyPIr9VEdbfs"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "    \n",
        "    features = {\n",
        "        'bias': 1.0, \n",
        "        # add some here\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit()\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        features.update({\n",
        "            # add something here\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper()\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        features.update({\n",
        "            # add something here\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper()\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, label in sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSnRyXFrdfJM"
      },
      "source": [
        "# explore quality for your new features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqIEo9oBJXDz"
      },
      "source": [
        "### Converting markup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBLwmVHJKsYr"
      },
      "source": [
        "Now it's time to get acquainted to NER markup or NER data labeling.<br>\n",
        "When we work with almost every NLP task, we usually need our data to be labeled. For NER problem data labeling is often rather expensive. Often we ask to label just in text, and then simple label all tokens for `BIO`-markup.<br>\n",
        "But in some tasks in which we need to very accurately define separate entities, the `BILUO`-markup may come to the rescue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe-STiqnkmv0"
      },
      "source": [
        "In our dataset we have `BIO-markup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zmuw8OzKe8m"
      },
      "source": [
        "#### Task 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpmHaJKgKgvR"
      },
      "source": [
        "write function to convert `BIO`-markup into `BILUO`-markup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4Z_T1nDk9l5"
      },
      "source": [
        "entities_list = [[mapping_[token] for token in tokens] for tokens in dataset_base[\"train\"]['ner_tags']]\n",
        "entities_list[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4BWseeEfZgm"
      },
      "source": [
        "# 1. B-PER I-PER -> B-PER L-PER\n",
        "# 2. O B-PER O -> O U-PER O\n",
        "# 3. B-PER I-PER I-PER -> B-PER I-PER L-PER\n",
        "# 4. O B-PER I-PER B-LOC -> O B-PER L-PER U-LOC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayuKKMMric90"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qql_NTIpktve"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def bio_2_biluo(entities_list, missing: str = 'O'):\n",
        "  result = list()\n",
        "  for entities in entities_list:\n",
        "    current_new_markup = [entities[0]]\n",
        "    # FILL code here\n",
        "    result.append(current_new_markup)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqSINs4MpNsk"
      },
      "source": [
        "assert len(bio_2_biluo(entities_list)) == len(entities_list)\n",
        "assert set(bio_2_biluo([entities_list[1]])[0]) == {'B-PER', 'L-PER'}\n",
        "assert len(set(bio_2_biluo([entities_list[7]])[0])) == 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZBBeLSdtlJZ"
      },
      "source": [
        "Sometimes after markup we have data labeled in offets: in plain text we get beginning and ending of each entity.<br>\n",
        "In this situations we can use function from spacy named `offsets_to_biluo_tags`. But you need to be careful, because sometimes it works incorrect. In this case you need to check translation of markup or write your own function to translate markups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnOZWlYhJcHF"
      },
      "source": [
        "Future readings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq7AOxcNVLDh"
      },
      "source": [
        "# you can also try to use spacy built-in ner model from spacy python library. Example of usage is here -> https://spacy.io/api/cli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mqTd1lnVLvQ"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZVLX9gSuNCh"
      },
      "source": [
        "In this part we will try to use some basic approaches to solve NER-task. Dataset will be the same as above. In this part don't forget to change runtime of your notebook to `GPU`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5gnC8Fn6_D_"
      },
      "source": [
        "!pip install spacy==3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLzibq6p8IMp"
      },
      "source": [
        "!pip install spacy-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-aR7upn26qV"
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import display, HTML\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import random\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import Counter\n",
        "from spacy.training import offsets_to_biluo_tags\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from torch import LongTensor, FloatTensor\n",
        "from torch.nn import functional as F\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMbmjRk1uuY7"
      },
      "source": [
        "Let's look at distribution of our data. Maybe we can deal with our problem by just using simple Neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmUSsRIfxY9B"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/snv-ds/NLP_course/master/week3/restauranttrain_updated.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUXGJVCju8X_"
      },
      "source": [
        "As we can see, there are not so many long texts. And we can forecast all tokens at ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_GtmCDLXKPB"
      },
      "source": [
        "### FCNN for NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jX73PIhur8i"
      },
      "source": [
        "For first approach we can just use basic FCNN. In production you will never see this, but for learning purpose it can be useful to explore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1AzMdIwwK9K"
      },
      "source": [
        "As usual, we will write to fix words order in our vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgQnTm9oAaF5"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import LongTensor, FloatTensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from typing import Any"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn1CcWyF4PkA"
      },
      "source": [
        "### to biluo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOqzHiTSbnot"
      },
      "source": [
        "This time we want to change our markup and train some models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09bbHjS_c9fZ"
      },
      "source": [
        "For whis purpose we will use spacy library. It contains built-in method that converts markup. But we need to correct it. That's why we wrote function that converts `BIO`-markup to `BILUO`-markup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fdftytA9tza"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URS5vbkg79mo"
      },
      "source": [
        "!python -m spacy init config base_config.cfg -p ner --force"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pOmC2XY9YU9"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")  # without vocabulary spacy can not work"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut4cXOBp2nom"
      },
      "source": [
        "with open('restauranttrain_updated.json', 'r') as f:\n",
        "    d = json.load(f)\n",
        "d[0]['paragraphs'][34]['sentences']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKP8sTSgdh8H"
      },
      "source": [
        "We will join our data, which is in list and then move it to spacy method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu19Un5g9YKc"
      },
      "source": [
        "tokens_dict = d[0]['paragraphs'][34]['sentences'][0]['tokens']\n",
        "tokens = [i['orth'] for i in tokens_dict]\n",
        "\n",
        "text = ' '.join(tokens)\n",
        "doc = nlp(text)\n",
        "entities = d[0]['paragraphs'][34]['entities']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCZydwx3dyUt"
      },
      "source": [
        "For futher usage you can download and use this function in your work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yMRq9F4jhNz"
      },
      "source": [
        "'qwerwer sdfsdfl sdfgkns sdgfdfgo Moscow 2002' -> [((23, 31), 'LOC'), ((33, 37), 'DATE')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9aV1nMi2ngA"
      },
      "source": [
        "from typing import List, Tuple, Union\n",
        "def convert_to_biluo(text: str = '',\n",
        "                     entities: List[Tuple] = None,\n",
        "                     tokens: list = None,\n",
        "                     missing: str = 'O') -> Tuple[Union[List[str], list, None], List[str]]:\n",
        "    \"\"\"\n",
        "    Tokenize text and return text tokens and ner labels.\n",
        "\n",
        "    Args:\n",
        "        text: text\n",
        "        entities: labels in spacy format\n",
        "        tokens: already tokenized text, if you want it\n",
        "        missing: lable for tokens without entities\n",
        "\n",
        "    Returns:\n",
        "        tokenized text and labels\n",
        "    \"\"\"\n",
        "\n",
        "    # create dicts with start/end position of token and its index\n",
        "    starts = []\n",
        "    ends = []\n",
        "    cur_index = 0\n",
        "    tokens = text.split() if tokens is None else tokens\n",
        "\n",
        "    for token in tokens:\n",
        "        starts.append(cur_index)\n",
        "        ends.append(cur_index + len(token))\n",
        "        cur_index += len(token) + 1\n",
        "\n",
        "    starts = {k: v for v, k in enumerate(starts)}\n",
        "    ends = {k: v for v, k in enumerate(ends)}\n",
        "\n",
        "    # this will be a list with token labels\n",
        "    biluo = [\"-\" for _ in text.split()]\n",
        "\n",
        "    # check that there are no overlapping entities\n",
        "    entities_indexes = [list(range(i[0], i[1])) for i in entities]\n",
        "    if max(Counter([i for j in entities_indexes for i in j]).values()) > 1:\n",
        "        raise ValueError('You have overlapping entities')\n",
        "\n",
        "    tokens_in_ents = {}\n",
        "\n",
        "    # Handle entity cases\n",
        "    for start_char, end_char, label in entities:\n",
        "        for token_index in range(start_char, end_char):\n",
        "            tokens_in_ents[token_index] = (start_char, end_char, label)\n",
        "        start_token = starts.get(start_char)\n",
        "        end_token = ends.get(end_char)\n",
        "        # Only interested if the tokenization is correct\n",
        "        if start_token is not None and end_token is not None:\n",
        "            if start_token == end_token:\n",
        "                biluo[start_token] = f\"U-{label}\"\n",
        "            else:\n",
        "                biluo[start_token] = f\"B-{label}\"\n",
        "                for i in range(start_token + 1, end_token):\n",
        "                    biluo[i] = f\"I-{label}\"\n",
        "                biluo[end_token] = f\"L-{label}\"\n",
        "\n",
        "    # put missing value for tokens without labels\n",
        "    entity_chars = set()\n",
        "    for start_char, end_char, label in entities:\n",
        "        for i in range(start_char, end_char):\n",
        "            entity_chars.add(i)\n",
        "\n",
        "    for ind, token in enumerate(tokens):\n",
        "        for i in range(list(starts.keys())[ind], list(ends.keys())[ind]):\n",
        "            if i in entity_chars:\n",
        "                break\n",
        "        else:\n",
        "            biluo[ind] = missing\n",
        "\n",
        "    return tokens, biluo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDTezl5X8zo7"
      },
      "source": [
        "# convert the data\n",
        "%%time\n",
        "new_data = []\n",
        "biluo_labels = []\n",
        "for i in range(len(d[0]['paragraphs'])):\n",
        "    tokens_dict = d[0]['paragraphs'][i]['sentences'][0]['tokens']\n",
        "    tokens = [i['orth'] for i in tokens_dict]\n",
        "    if len([i['orth'] for i in tokens_dict]) > 1:\n",
        "        \n",
        "        text = ' '.join(tokens)\n",
        "        doc = nlp(text)\n",
        "        entities = d[0]['paragraphs'][i]['entities']\n",
        "\n",
        "        new_ents = offsets_to_biluo_tags(doc, entities)  # using spacy function\n",
        "        if entities == []:\n",
        "            new_ents = ['O'] * len(tokens)\n",
        "        new_data.append(tokens)\n",
        "        \n",
        "        biluo_labels.append(new_ents)\n",
        "        if len(tokens) != len(new_ents): # if lists from 2 methods don't match\n",
        "            \n",
        "            ents2 = convert_to_biluo(text, entities)[1]\n",
        "            biluo_labels[-1] = ents2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ttzrWaGF_Qt"
      },
      "source": [
        "biluo_labels[0], new_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI5IdnZFcli6"
      },
      "source": [
        "max_lens = list()\n",
        "for row in new_data:\n",
        "    max_lens.append(len(row))\n",
        "max_lens = pd.Series(max_lens)\n",
        "max_lens.plot();\n",
        "max_lens.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF_88li06w_A"
      },
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "from typing import List, Tuple, Union, Dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tRh7QQLHV_H"
      },
      "source": [
        "#### Task 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coZ8RhAfeXCp"
      },
      "source": [
        "create to variables, that will contains mappings between entities and indices. Each dictionary must include entities: `O` and `PAD`.\n",
        "Initialize variable `tag_to_idx`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EavWQqK-aVf"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "tags = sorted(list({i for j in biluo_labels for i in j}))\n",
        "\n",
        "tag_to_idx = {}\n",
        "\n",
        "with open('mapping.json', 'w') as f:\n",
        "  json.dump(tag_to_idx, f)\n",
        "\n",
        "idx_to_tag = {second: first for first, second in tag_to_idx.items()}\n",
        "\n",
        "tag_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ITsQT-SIWB5"
      },
      "source": [
        "def get_word_to_idx(count: List[Tuple[str, int]],\n",
        "                   min_words: Union[int, float] = 0.0,\n",
        "                   max_words: Union[int, float] = 1.0) -> Dict[str, int]:\n",
        "    max_count = count[0][1]\n",
        "    if isinstance(min_words, float):\n",
        "        min_words = max_count * min_words\n",
        "    if isinstance(max_words, float):\n",
        "        max_words = max_count * max_words\n",
        "    \n",
        "    all_words = [w[0] for w in count if max_words >= w[1] >= min_words]\n",
        "    \n",
        "    all_words = ['<pad>', '<unk>'] + all_words\n",
        "    \n",
        "    word_to_idx = {k: v for k, v in zip(all_words, range(0, len(all_words)))}\n",
        "    return word_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsMgDEWoeaEC"
      },
      "source": [
        "#### Task 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF2wHVhkeYtW"
      },
      "source": [
        "Count how many unique words are there in our train dataset. Parameters `min_words` and `max_words` should be initialized as default values. Initialize variable `word_to_idx` from method `get_word_to_idx`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKD4GgGpfybD"
      },
      "source": [
        "##### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J9Bf3xof0eB"
      },
      "source": [
        "# 1. first you can count occurences of each word\n",
        "# 2. second you can pass list of tuples for each pair (word, num_of_occurencies) to function get_word_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGSXKAasgIf6"
      },
      "source": [
        "##### Continue work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE62qt8k-ad-"
      },
      "source": [
        "count = Counter()\n",
        "word_to_idx = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-EIfsqqfNtZ"
      },
      "source": [
        "assert len(word_to_idx) == 3805"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7mJ29v2IV1-"
      },
      "source": [
        "def create_matrix_of_texts(dataset, max_sequence_length, \n",
        "                           pad_token, word2index, unk_token):\n",
        "    texts = np.full((len(dataset), max_sequence_length),\n",
        "                    word2index[pad_token], dtype=np.int64)  # creating empty matrix\n",
        "\n",
        "    for ind, row in enumerate(dataset):\n",
        "          trim_length = min(max_sequence_length, len(row))\n",
        "          text = row[:trim_length]\n",
        "          texts[ind, :trim_length] = [word2index.get(item.lower(),\n",
        "                                                     word2index[unk_token]) for item in text]\n",
        "    return texts\n",
        "\n",
        "def create_matrix_of_tags(dataset, max_sequence_length, pad_index, tag2idx):\n",
        "    tags = np.full((len(dataset), max_sequence_length),\n",
        "                    pad_index, dtype=np.int64)  # creating empty matrix\n",
        "\n",
        "    for ind, row in enumerate(dataset):\n",
        "          trim_length = min(max_sequence_length, len(row))\n",
        "          labels = row[: trim_length]\n",
        "          tags[ind, : trim_length] = [tag2idx[item] for item in labels]\n",
        "    return tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3qdl6XJHq6z"
      },
      "source": [
        "texts = create_matrix_of_texts(new_data, \n",
        "                               int(max_lens.quantile(0.97)),\n",
        "                               '<pad>', word_to_idx)\n",
        "tags = create_matrix_of_tags(biluo_labels,\n",
        "                             int(max_lens.quantile(0.97)),\n",
        "                             tag_to_idx['PAD'],\n",
        "                             tag_to_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69okXnDhKpYW"
      },
      "source": [
        "class NerDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 texts: np.array,\n",
        "                 tags: np.array):\n",
        "        self.tags = tags\n",
        "        self.texts = texts\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "        tokens_tensor = torch.tensor(self.texts[idx], dtype=torch.int64)\n",
        "        return tokens_tensor, torch.tensor(self.tags[idx], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        dataset_len = self.texts.shape[0]\n",
        "        return dataset_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCe1keAYKpMb"
      },
      "source": [
        "ner_dataset = NerDataset(texts, tags)\n",
        "assert len(ner_dataset) == 7634"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG6oV-zTKo_e"
      },
      "source": [
        "from torch.utils.data.dataset import random_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXx7nlGeGo-R"
      },
      "source": [
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGrEjzG4gadC"
      },
      "source": [
        "#### Task 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8MniGMFgdlG"
      },
      "source": [
        "Initialize dataloaders for train and validation. There is no need to shuffle validation dataloader, but it is better to shuffle train and drop last batch from train dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehjV96FzLItx"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "num_train = int(len(ner_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(ner_dataset, [num_train, len(ner_dataset) - num_train])\n",
        "\n",
        "train_dataloader = pass\n",
        "valid_dataloader = pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl8EJ-bzgz7t"
      },
      "source": [
        "In this toy example we will first try simple FCNN for your problem. Let's look how bad/good it fits our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWLYR4MFhWDB"
      },
      "source": [
        "#### Task 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10wgebnyhYiL"
      },
      "source": [
        "Initialize sequantial layers of our FCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ngc4mz_P7a5"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "class NerModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        word2idx: Dict,\n",
        "        embedding_dim: int = 100,\n",
        "        mapping: Dict[int, str] = None,\n",
        "        hidden_size: int = 256,\n",
        "    ):\n",
        "        super(NerModel, self).__init__()\n",
        "        if not mapping:\n",
        "            raise RuntimeError(f'Empty labels')\n",
        "        self.word2idx = word2idx\n",
        "        self.labels = mapping\n",
        "\n",
        "        self.linear_sigmoid_stack = nn.Sequential(\n",
        "            # FILL YOUR CODE HERE\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens: LongTensor) -> FloatTensor:\n",
        "\n",
        "        return self.linear_sigmoid_stack(tokens).view(-1, len(self.labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il0TbOeVhkZC"
      },
      "source": [
        "Now we will create basic network and check how it calculate loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyw63e8YP_ko"
      },
      "source": [
        "model = NerModel(word_to_idx, 100, {idx: str(idx) for idx in range(10)})\n",
        "assert (\n",
        "        len(list(name for name, module in model.named_modules())) > 3\n",
        "    ), \"Not enough layers created\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QktELcGQEIw"
      },
      "source": [
        "num_classes = len(tag_to_idx)\n",
        "model = NerModel(word_to_idx, 30, {idx: str(idx) for idx in range(num_classes)})\n",
        "seq_len = 32\n",
        "example_input = torch.randint(0, 2, (BATCH_SIZE, seq_len), dtype=torch.int64)\n",
        "logits = model(example_input)\n",
        "assert isinstance(logits, torch.FloatTensor)\n",
        "assert logits.shape == (BATCH_SIZE * seq_len, num_classes), f\"current size of model output {logits.shape}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wsrqGA7hz1O"
      },
      "source": [
        "i = iter(train_dataloader)\n",
        "text, label = next(i)\n",
        "logits = model(text)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "loss_function(logits, label.view(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaI3WuNFh3mK"
      },
      "source": [
        "Everything looks pretty well and seems correct. Lets now write evaluation function and begin our training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rENMRVxiJdR"
      },
      "source": [
        "#### Task 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6VN-xHQiNNY"
      },
      "source": [
        "Fill lines of code. First you need to initialize variable of `correct_labels` (labels, that are not special ones). Then you need to get `true_labels` and `predicted` variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rXHLysLiL3c"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    correct_labels = None  # Fill your code here\n",
        "    predicted, true_labels = list(), list()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(dataloader):\n",
        "            tokens, label = batch\n",
        "            tokens = tokens.to(device)\n",
        "            \n",
        "            logits = model(tokens)\n",
        "            predictions = F.log_softmax(logits, dim=1).reshape(-1,\n",
        "                                                               int(max_lens.quantile(0.97)),\n",
        "                                                               len(tag_to_idx)).argmax(dim=2).flatten().detach().cpu().numpy()\n",
        "            predicted.extend(predictions)\n",
        "            true_labels.extend(label.flatten().detach().cpu().numpy())\n",
        "    \n",
        "    true_labels = None # Fill your code here\n",
        "    \n",
        "    predicted = None  # Fill your code here\n",
        "    print('\\n', classification_report(true_labels,\n",
        "                                      predicted,\n",
        "                                      labels=correct_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKo2r1WYizlt"
      },
      "source": [
        "Now we can create our model and start trainig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ULTTPvri4YI"
      },
      "source": [
        "model = NerModel(word_to_idx, 300, tag_to_idx)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXa9nIHGi5uV"
      },
      "source": [
        "for e in range(6):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    for sent in tqdm(train_dataloader):\n",
        "\n",
        "            # (1) Set gradient to zero for new example: Set gradients to zero before pass\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # (2) Encode sentence and tag sequence as sequences of indices\n",
        "            input_sent, gold_tags = sent\n",
        "\n",
        "            # (3) Predict tags (sentence by sentence)\n",
        "            if len(input_sent) > 0:\n",
        "                pred_scores = model(input_sent.to(device))\n",
        "                mask = gold_tags != 0\n",
        "                # (4) Compute loss and do backward step\n",
        "                loss = loss_function(pred_scores.to(device), gold_tags.view(-1).to(device))\n",
        "                loss.backward()\n",
        "              \n",
        "                # (5) Optimize parameter values\n",
        "                optimizer.step()\n",
        "          \n",
        "                # (6) Accumulate loss\n",
        "                total_loss += loss\n",
        "    print('\\nEpoch: %d, loss: %.4f' % (e, total_loss / len(train_dataloader)))\n",
        "    evaluate(valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sshz2tYrjBy9"
      },
      "source": [
        "We did it, but quality of model is rather bad. Now you can try RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MxVkYxXjK8c"
      },
      "source": [
        "### RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-pn2aRRjS_D"
      },
      "source": [
        "All process from FCNN works fine, but we need to use new architecture. Let's write new model, that process data and uses some kind of Recurrent Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrEZNuAjjniz"
      },
      "source": [
        "#### Task 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd6h4WsUjpzB"
      },
      "source": [
        "Fill missing layers of model. You can use any RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhyuQhs1LIiI"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "class NerRNNModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        word2idx: Dict,\n",
        "        embedding_dim: int = 100,\n",
        "        mapping: Dict[int, str] = None,\n",
        "        hidden_size: int = 256\n",
        "    ):\n",
        "        super(NerRNNModel, self).__init__()\n",
        "        if not mapping:\n",
        "            raise RuntimeError(f'Empty labels')\n",
        "        self.word2idx = word2idx\n",
        "        self.labels = mapping\n",
        "        self.embedding = nn.Embedding(len(word_to_idx), embedding_dim)\n",
        "        self.encoder = nn.RNN(\n",
        "            # FILL YOUR CODE HERE\n",
        "        )\n",
        "        self.projection = nn.Linear(hidden_size, len(mapping))\n",
        "\n",
        "    def forward(self, tokens: LongTensor) -> FloatTensor:\n",
        "\n",
        "        emb = self.embedding(tokens)        \n",
        "        h, _ = self.encoder(emb)\n",
        "        pred = self.projection(h)\n",
        "        return pred.view(-1, len(self.labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHr5vn8Xj99l"
      },
      "source": [
        "Now we can duplicate all cells from above and simply just start new iteration of training new model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imiTzbYgLQEx"
      },
      "source": [
        "model = NerRNNModel(word_to_idx, 100, {idx: str(idx) for idx in range(10)})\n",
        "assert (\n",
        "        len(list(name for name, module in model.named_modules())) > 3\n",
        "    ), \"Not enough layers created\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM-Ti_5cLP3V"
      },
      "source": [
        "num_classes = len(tag_to_idx)\n",
        "model = NerRNNModel(word_to_idx, 30, {idx: str(idx) for idx in range(num_classes)})\n",
        "seq_len = 32\n",
        "example_input = torch.randint(0, 2, (BATCH_SIZE, seq_len), dtype=torch.int64)\n",
        "logits = model(example_input)\n",
        "assert isinstance(logits, torch.FloatTensor)\n",
        "assert logits.shape == (BATCH_SIZE * seq_len, num_classes), f\"current size of model output {logits.shape}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lDFmLHDLPox"
      },
      "source": [
        "i = iter(train_dataloader)\n",
        "text, label = next(i)\n",
        "logits = model(text)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "loss_function(logits, label.view(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCaivj5uLiHA"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    correct_labels = [value for value in idx_to_tag.values() if value != 'O' and value != 'PAD']\n",
        "    predicted, true_labels = list(), list()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(dataloader):\n",
        "            tokens, label = batch\n",
        "            tokens = tokens.to(device)\n",
        "            \n",
        "            logits = model(tokens)\n",
        "            predictions = F.log_softmax(logits, dim=1).reshape(-1,\n",
        "                                                               int(max_lens.quantile(0.97)),\n",
        "                                                               len(tag_to_idx)).argmax(dim=2).flatten().detach().cpu().numpy()\n",
        "            predicted.extend(predictions)\n",
        "            true_labels.extend(label.flatten().detach().cpu().numpy())\n",
        "    \n",
        "    true_labels = [idx_to_tag[val] for val in true_labels]\n",
        "    \n",
        "    predicted = [idx_to_tag[val] for val in predicted]\n",
        "    print('\\n', classification_report(true_labels,\n",
        "                                      predicted,\n",
        "                                      labels=correct_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LtiUtB5Lh5R"
      },
      "source": [
        "model = NerRNNModel(word_to_idx, 300, tag_to_idx)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spM7aX3ELhsC"
      },
      "source": [
        "sum([params.numel() for params in model.parameters() if params.requires_grad])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRf8qs1fLIVY"
      },
      "source": [
        "for e in range(6):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    for sent in tqdm(train_dataloader):\n",
        "\n",
        "            # (1) Set gradient to zero for new example: Set gradients to zero before pass\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # (2) Encode sentence and tag sequence as sequences of indices\n",
        "            input_sent, gold_tags = sent\n",
        "\n",
        "            # (3) Predict tags (sentence by sentence)\n",
        "            if len(input_sent) > 0:\n",
        "                pred_scores = model(input_sent.to(device))\n",
        "                mask = gold_tags != 0\n",
        "                # (4) Compute loss and do backward step\n",
        "                loss = loss_function(pred_scores.to(device), gold_tags.view(-1).to(device))\n",
        "                loss.backward()\n",
        "              \n",
        "                # (5) Optimize parameter values\n",
        "                optimizer.step()\n",
        "          \n",
        "                # (6) Accumulate loss\n",
        "                total_loss += loss\n",
        "    print('\\nEpoch: %d, loss: %.4f' % (e, total_loss / len(train_dataloader)))\n",
        "    evaluate(valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkBKi5YtlpsT"
      },
      "source": [
        "Futher working:\n",
        "- Try more complex architecture\n",
        "- try bidirectional rnns\n",
        "- try other hyperparameters\n",
        "- try pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02pZb8CdmXNB"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}