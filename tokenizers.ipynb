{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wizard339/education/blob/main/tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI_L2TBSZadk"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s09CQfFyURY8"
      },
      "source": [
        "# ðŸ¤— Tokenizers\n",
        "\n",
        "Ð”Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼ Ð½Ð°Ð¼ Ð½ÑƒÐ¶Ð½Ð¾ ÑƒÐ¼ÐµÑ‚ÑŒ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÑ‚ÑŒ ÐµÐ³Ð¾ Ð² Ð¿Ð¾Ð½ÑÑ‚Ð½Ð¾Ð¼ ÐºÐ¾Ð¿Ð¼ÑŒÑŽÑ‚ÐµÑ€Ñƒ Ð²Ð¸Ð´Ðµ Ð¸ Ð¿ÐµÑ€Ð²Ñ‹Ðµ ÑˆÐ°Ð³Ð¸ Ð² Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ð¸ - ÑÑ‚Ð¾ Ñ€Ð°Ð·Ð±Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑÑ‚ Ð½Ð° Ñ‚Ð¾ÐºÐµÐ½Ñ‹ (ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð³ÑƒÑ‚ Ð±Ñ‹Ñ‚ÑŒ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸, Ð° Ð¼Ð¾Ð³ÑƒÑ‚ Ð¸ Ð½Ðµ Ð±Ñ‹Ñ‚ÑŒ Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ð¼ÐµÑ‚Ð¾Ð´Ð° Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸) Ð¸ ÑÐ¾Ð¿Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ ÑÐ»Ð¾Ð²Ñƒ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð½Ð¾Ð¼ÐµÑ€ Ð¾Ñ‚ 0 Ð´Ð¾ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° Ð²Ð°ÑˆÐµÐ³Ð¾ ÑÐ»Ð¾Ð²Ð°Ñ€Ñ. ÐŸÐ¾ÑÐ»Ðµ ÑÑ‚Ð¾Ð³Ð¾ Ð¼Ñ‹ ÑÐ¼Ð¾Ð¶ÐµÐ¼ ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑÑ‚Ñ€Ð¾ÐºÑƒ Ñ‚Ð¸Ð¿Ð° \"ÑÑ‚Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ°\" Ð² Ð²ÐµÐºÑ‚Ð¾Ñ€ Ð²Ð¸Ð´Ð° `[47, 392, 38]`. Ð­Ñ‚Ð° Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ñ - Ð·Ð°Ð¼ÐµÐ½Ñ‹ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð½Ð° Ð¸Ñ… Ð¸Ð½Ð´ÐµÐºÑÑ‹ - Ñ‡Ð°ÑÑ‚Ð¾ Ð½Ð°Ð·Ñ‹Ð²Ð°ÐµÑ‚ÑÑ **Ð½ÑƒÐ¼ÐµÑ€Ð¸ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ**, Ð° Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð°Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ñ - **Ð´ÐµÐ½ÑƒÐ¼ÐµÑ€Ð¸ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ**.\n",
        "\n",
        "Ð•ÑÐ»Ð¸ Ð²Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚Ðµ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ `sklearn.feature_extraction.text`, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð´Ð»Ñ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ `TfidfVectorizer`, sklearn Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð²ÑÐµ ÑÑ‚Ð¸ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð·Ð° Ð²Ð°Ñ Ð¿Ð¾Ð´ ÐºÐ°Ð¿Ð¾Ñ‚Ð¾Ð¼, Ð½Ð¾ ÐµÑÐ»Ð¸ Ð²Ñ‹ Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚Ð¸, Ð¶Ð¸Ð·Ð½ÑŒ ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑÑ Ð³Ð¾Ñ€Ð°Ð·Ð´Ð¾ ÑÐ»Ð¾Ð¶Ð½ÐµÐµ Ð¸ Ð´Ð¾ Ð½ÐµÐ´Ð°Ð²Ð½ÐµÐ³Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¿Ð¾Ñ‡Ñ‚Ð¸ Ð²ÑÐµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð»Ð¸ ÑÐ°Ð¼Ð¾Ð¿Ð¸ÑÐ½Ñ‹Ðµ ÑˆÑ‚ÑƒÐºÐ¸, Ñ‡Ñ‚Ð¾Ð±Ñ‹ (Ð´Ðµ)Ð½ÑƒÐ¼ÐµÑ€Ð¸ÐºÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ñ‚ÑŒ Ñ‚ÐµÐºÑÑ‚Ñ‹. Ð­Ñ‚Ð¾ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ð»Ð¾ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²Ð¾ Ð½ÐµÑƒÐ´Ð¾Ð±ÑÑ‚ ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ñ… Ñ Ñ‚ÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð²Ð¾-Ð¿ÐµÑ€Ð²Ñ‹Ñ… Ð²Ð°Ð¼ Ð¿Ñ€Ð¸Ñ…Ð¾Ð´Ð¸Ð»Ð¾ÑÑŒ Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð±Ð¾Ð»ÑŒÑˆÐµ ÐºÐ¾Ð´Ð° (Ð¸ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð¾ÑˆÐ¸Ð±Ð¾Ðº), Ð²Ð¾-Ð²Ñ‚Ð¾Ñ€Ñ‹Ñ… Ñ€ÑÐ´Ð¾Ð¼ Ñ Ð½ÑƒÐ¼ÐµÑ€Ð¸ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹ Ð»ÐµÐ¶Ð¸Ñ‚ Ð¾Ñ‡ÐµÐ½ÑŒ Ð¼Ð½Ð¾Ð³Ð¾ Ð¼ÐµÐ»ÐºÐ¸Ñ… Ð¿Ð¾Ð´Ð·Ð°Ð´Ð°Ñ‡ Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð², Ñ‚Ð°ÐºÐ¸Ñ… ÐºÐ°Ðº UNK, ÑÐµÑ€Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²Ð°ÑˆÐ¸Ñ… Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð², Ð¸ Ñ€Ð°ÑÐ¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð½Ð° Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð² Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ. \n",
        "\n",
        "Huggingface Tokenizers Ð¿Ð¾ÑÑ‚ÐµÐ¿ÐµÐ½Ð½Ð¾ ÑÑ‚Ð°Ð½Ð¾Ð²ÑÑ‚ÑÑ Ð²ÑÑ‘ Ð±Ð¾Ð»ÐµÐµ Ð¸ Ð±Ð¾Ð»ÐµÐµ Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ð¼Ð¸ Ð² NLP-ÑÐ¾Ð¾Ð±Ñ‰ÐµÑÑ‚Ð²Ðµ. Ð­Ñ‚Ð° Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ° Ð±Ñ‹ÑÑ‚Ñ€Ð¾ Ñ€Ð°Ð·Ð²Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð¸ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð²Ð°Ð¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ - Ð¾Ñ‚ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ð½Ð° ÑÐ»Ð¾Ð²Ð° Ð¿Ð¾ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñƒ Ð´Ð¾ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ð½Ð° Ð±Ð¸Ñ‚Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ unicode-ÑÑ‚Ñ€Ð¾Ðº Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹ Ð±Ð¸Ñ‚Ð¾Ð²Ð¾Ð¹ n-gram. Ð¢Ð°ÐºÐ¶Ðµ Ð¾Ð½Ð° ÑƒÐ¼ÐµÐµÑ‚ Ð´ÐµÐ»Ð°Ñ‚ÑŒ Ð½ÑƒÐ¼ÐµÑ€Ð¸ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð¸ Ð¾Ñ‡ÐµÐ½ÑŒ Ð±Ñ‹ÑÑ‚Ñ€Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚. Ð’Ð¼ÐµÑÑ‚Ð¾ ÑÐ°Ð¼Ð¾Ð¿Ð¸ÑÐ½Ñ‹Ñ… Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð²ÑÑ‘ ÐµÑ‰Ñ‘ Ñ‡Ð°ÑÑ‚Ð¾ Ð¿Ñ€ÐµÐ¿Ð¾Ð´Ð°ÑŽÑ‚ÑÑ Ð² ÐºÑƒÑ€ÑÐ°Ñ… Ð¿Ð¾ NLP, Ð¼Ñ‹ Ð±ÑƒÐ´ÐµÐ¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ ðŸ¤— Tokenizers Ð² Ñ‚ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð²ÑÐµÐ³Ð¾ ÐºÑƒÑ€ÑÐ° Ð¸ Ð´Ð¾Ð²Ð¾Ð»ÑŒÐ½Ð¾ Ð±Ñ‹ÑÑ‚Ñ€Ð¾ Ð²Ñ‹ Ð½Ð°ÑƒÑ‡Ð¸Ñ‚ÐµÑÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ð´Ð°Ð¶Ðµ Ð½Ðµ ÑÐ°Ð¼Ñ‹Ð¼Ð¸ Ð¾Ñ‡ÐµÐ²Ð¸Ð´Ð½Ñ‹Ð¼Ð¸ ÐµÑ‘ Ð¼ÐµÑ‚Ð¾Ð´Ð°Ð¼Ð¸. Ð ÑÐµÐ¹Ñ‡Ð°Ñ Ð¼Ñ‹ Ð½Ð°Ñ‡Ð½Ñ‘Ð¼ Ñ ÑÐ°Ð¼Ñ‹Ñ… Ð¾ÑÐ½Ð¾Ð²."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DLYms6MBUPYm",
        "outputId": "e3f433ce-fadd-44c1-f415-36e6a6703a02"
      },
      "source": [
        "import datasets\n",
        "import tokenizers\n",
        "tokenizers.__version__  # should be above or equal 0.10.0rc1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.13.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww0-lHJwaSMr"
      },
      "source": [
        "Ð—Ð° Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ñ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ñ‚ÐµÐºÑÑ‚Ð°, Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ ÐµÐ³Ð¾ Ð½Ð° Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ð¸ Ð½ÑƒÐ¼ÐµÑ€Ð¸ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð² ðŸ¤— Tokenizers Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑ‚ Ð¾Ð±ÑŠÐµÐºÑ‚ `Tokenizer`. Ð¡Ð°Ð¼Ñ‹Ð¹ Ð½Ð°Ð³Ð»ÑÐ´Ð½Ñ‹Ð¹ ÑÐ¿Ð¾ÑÐ¾Ð± ÐµÐ³Ð¾ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ - ÑÑ‚Ð¾ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ ÐµÐ¼Ñƒ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ, Ð¼Ð°Ð¿ÑÑ‰Ð¸Ð¹ ÑÐ»Ð¾Ð²Ð° Ð½Ð° Ð¸Ñ… Ð¸Ð½Ð´ÐµÐºÑÑ‹. Ð”Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ð½Ð°Ð¿Ð¸ÑˆÐµÐ¼ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÑÑ‚Ð¾ Ð´ÐµÐ»Ð°ÐµÑ‚.\n",
        "\n",
        "**NOTE:** Ð¡Ð°Ð¼Ð°Ñ Ð¿Ñ€Ð¾ÑÑ‚Ð°Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ - Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð½Ð° ÑÐ»Ð¾Ð²Ð° Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ regex - ÑÑ‚Ð¾Ð¸Ñ‚ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐ±Ð¾ÐºÑƒ Ð² Tokenizers Ð¸ Ð´Ð°Ð¶Ðµ Ð½Ðµ Ð½Ð°Ð·Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð¼ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸, Ñ‚Ðº Ð·Ð°Ñ‡Ð°ÑÑ‚ÑƒÑŽ Ð¾Ð½Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÐºÐ°Ðº Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÑˆÐ°Ð³ Ð±Ð¾Ð»ÐµÐµ ÑÐ»Ð¾Ð¶Ð½Ð¾Ð³Ð¾ Ð¼ÐµÑ‚Ð¾Ð´Ð° Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸. ÐŸÐ¾ÑÑ‚Ð¾Ð¼Ñƒ Ð¾Ð½Ð° Ð·Ð°Ð´Ð°Ñ‘Ñ‚ÑÑ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¾Ð±ÑŠÐµÐºÑ‚Ð° Ð²Ð¸Ð´Ð° `pre_tokenizer`.\n",
        "\n",
        "Ð”Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð¼Ñ‹ Ð´ÐµÐ»Ð°ÐµÐ¼ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ ÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð¾Ð±ÑŠÐµÐºÑ‚ `pre_tokenizer` Ð¸ Ð¿Ñ€ÐµÐ¿Ð¾Ñ†ÐµÑÑÐ¸Ñ‚ Ð½Ð°ÑˆÐ¸ Ñ‚ÐµÐºÑÑ‚Ñ‹. ÐŸÐ¾ÑÐ»Ðµ Ñ‡ÐµÐ³Ð¾ Ð¼Ñ‹ Ð¿Ð¾Ð´ÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹ Ð½Ð°ÑˆÐ¸Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð¸ Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÑ‚ top-N ÑÐ°Ð¼Ñ‹Ñ… Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð½Ñ‹Ñ…, ÑÑ‚Ð¾ ÑƒÐ´Ð¾Ð±Ð½Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¾Ð±ÑŠÐµÐºÑ‚Ð° `Counter` Ð¸Ð· Ð²ÑÑ‚Ñ€Ð¾ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð² Python Ð¼Ð¾Ð´ÑƒÐ»Ñ collections. Ð’ Ð¸Ð³Ñ€ÑƒÑˆÐµÑ‡Ð½Ð¾Ð¼ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ðµ Ð² ÑÐ»ÐµÑ‰ÑƒÑŽÑ‰ÐµÐ¹ ÑÑ‡ÐµÐ¹ÐºÐµ, ÑÐ¾ÑÑ‚Ð¾ÑÑ‰Ð¸Ð¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð»Ð¸ÑˆÑŒ Ð¸Ð· Ð´Ð²ÑƒÑ… ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ñ… Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹ ÑÑ‚Ð° Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ñ Ð½ÐµÐ²Ð°Ð¶Ð½Ð°, Ð½Ð¾ ÐºÐ¾Ð³Ð´Ð° Ð²Ñ‹ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚Ðµ Ñ Ñ‚ÐµÐºÑÑ‚Ð°Ð¼Ð¸ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð°Ð¼Ð¸ Ñ Ð’Ð¸ÐºÐ¸Ð¿ÐµÐ´Ð¸ÑŽ, Ð²Ð°Ñˆ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ð±Ñ‹ÑÑ‚Ñ€Ð¾ ÑÑ‚Ð°Ð½ÐµÑ‚ Ð½ÐµÐ¾Ð±ÑŠÑ‘Ð¼Ð½Ñ‹Ñ… Ñ€Ð°Ð·Ð¼ÐµÑ€Ð¾Ð² Ð¸ ÐµÐ³Ð¾ Ð½ÑƒÐ¶Ð½Ð¾ ÑƒÐ¼ÐµÑ‚ÑŒ Ð¾Ð³Ñ€Ð½Ð¸Ñ‡Ð¸Ð²Ð°Ñ‚ÑŒ.\n",
        "\n",
        "Ð¢Ñ€ÐµÑ‚Ð¸Ð¹ ÑˆÐ°Ð³ - ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¼Ð°Ð¿Ð¸Ð½Ð³Ð° ÑÐ»Ð¾Ð² Ð½Ð° Ð¸Ñ… Ð¸Ð½Ð´ÐµÐºÑÑ‹. ÐžÐ½ Ð´Ð¾Ð²Ð¾Ð»ÑŒÐ½Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¸ ÐµÐ³Ð¾ Ð¼Ð¾Ð¶Ð½Ð¾ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ Ð² Ð¾Ð´Ð½Ñƒ ÑÑ‚Ñ€Ð¾Ñ‡ÐºÑƒ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ dictionary comprehension. Ð˜ Ð¿Ð¾ÑÐ»Ðµ ÑÑ‚Ð¾Ð³Ð¾ Ð¼Ñ‹ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð¾Ð±ÑŠÐµÐºÑ‚ `Tokenizer` Ð¸ Ð¿Ñ€Ð¸ÑÐ²Ð¾Ð¸Ñ‚ÑŒ ÐµÐ¼Ñƒ Ð¿Ñ€ÐµÑ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¼Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð»Ð¸ Ð´Ð»Ñ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð° Ð½Ð° ÑÐ»Ð¾Ð²Ð°."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPZ9Exn3ZeKa"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "texts = ['a list of sentences from my dataset', 'this is a text with known words']\n",
        "\n",
        "\n",
        "def make_whitespace_tokenizer(texts, max_vocab_size=10_000, unk_token='UNK'):\n",
        "    pre_tokenizer = Whitespace()\n",
        "    tokenized_texts = [[w for w, _ in pre_tokenizer.pre_tokenize_str(t)] for t in texts]\n",
        "\n",
        "    c = Counter()\n",
        "    for text in tokenized_texts:\n",
        "        c.update(text)\n",
        "\n",
        "    token2id = {word: i + 1 for i, (word, count) in enumerate(c.most_common(max_vocab_size))}\n",
        "\n",
        "    # usually, UNK is assigned index 0 or 1\n",
        "    token2id[unk_token] = 0\n",
        "\n",
        "    tokenizer = tokenizers.Tokenizer(WordLevel(token2id, unk_token))\n",
        "    tokenizer.pre_tokenizer = pre_tokenizer\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "tokenizer = make_whitespace_tokenizer(texts)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpbn6P6Qd2Jq"
      },
      "source": [
        "# Encoding and decoding text\n",
        "\n",
        "Ð”Ð»Ñ Ð½ÑƒÐ¼ÐµÑ€Ð¸ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ‚ÐµÐºÑÑ‚Ð° Ð¼Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¼ÐµÑ‚Ð¾Ð´ `.encode()`, Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÑŽÑ‰Ð¸Ð¹ Ð¾Ð±ÑŠÐµÐºÑ‚ ÐºÐ»Ð°ÑÑÐ° `Encoding`. ÐžÐ½ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð² ÑÐµÐ±Ðµ Ð¼Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÐµÐ·Ð½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸, Ð½Ð¾ ÑÐ°Ð¼Ð¾Ðµ Ð³Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¸Ð· Ð½ÐµÐ³Ð¾ - `.ids` Ð´Ð°ÑŽÑ‰Ð¸Ð¹ Ð½Ð°Ð¼ Ð½ÑƒÐ¼ÐµÑ€Ð¸ÐºÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚, Ð³Ð´Ðµ ÑÐ»Ð¾Ð²Ð° Ð·Ð°Ð¼ÐµÐ½ÐµÐ½Ñ‹ Ð½Ð° Ð¸Ñ… Ð¸Ð½Ð´ÐµÐºÑÑ‹."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rF_Ouq4bg8k",
        "outputId": "159b3e3f-6447-473a-94b5-f78fa0e6aa90"
      },
      "source": [
        "_text = 'this is a text with unknown_word'\n",
        "e = tokenizer.encode(_text)\n",
        "e"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM1wIMLIiFD_",
        "outputId": "270aebac-fe11-41c7-adc0-0060e5344b3f"
      },
      "source": [
        "e.ids"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 9, 1, 10, 11, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEb3MVLueR8V"
      },
      "source": [
        "Ð”ÐµÐ½ÑƒÐ¼ÐµÑ€Ð¸ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´ÐµÐ»Ð°ÐµÑ‚ÑÑ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð¼ `.decode()` (Ð¾Ð±Ñ€Ð°Ñ‚Ð¸Ñ‚Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ, Ñ‡Ñ‚Ð¾ Ð½Ð° Ð²Ñ…Ð¾Ð´ Ðº Ð½ÐµÐ¼Ñƒ Ð¿Ñ€Ð¸Ñ…Ð¾Ð´ÑÑ‚ `.ids`, Ð° Ð½Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚ `Encoding`). Ð­Ñ‚Ð° Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‡ÐµÐ½ÑŒ Ð²Ð°Ð¶Ð½Ð° Ð½Ð° Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐµ Ð¿Ð¾ Ð¼Ð½Ð¾Ð³Ð¸Ð¼ Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ð°Ð¼ Ð¸ Ð¾Ð´Ð½Ð° Ð¸Ð· Ð½Ð¸Ñ… - Ð²Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð½Ð° Ñ‚ÐµÐºÑÑ‚ Ñ‚Ð°Ðº, ÐºÐ°Ðº ÐµÐ³Ð¾ Ð²Ð¸Ð´Ð¸Ñ‚ Ð²Ð°ÑˆÐ° Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÑŒ - Ñ Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¸ ÑÐ»Ð¾Ð°Ð²Ð¼Ð¸ Ð·Ð°Ð¼ÐµÐ½ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð½Ð° Ñ‚Ð¾ÐºÐµÐ½ UNK Ð¸, Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾, Ñ Ð²Ð°Ð¼Ð¸ÑˆÐ¸ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ð¼Ð¸ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³Ð°. Ð­Ñ‚Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ð¾Ñ‡ÐµÐ½ÑŒ Ñ‡Ð°ÑÑ‚Ñ‹ Ð½Ð° Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐµ Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´ÐµÐ½ÑƒÐ¼ÐµÑ€Ð¸ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÑÐ¸Ð»ÑŒÐ½Ð¾ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ Ð² Ð´ÐµÐ±Ð°Ð³Ðµ.\n",
        "\n",
        "ÐšÑ€Ð¾Ð¼Ðµ ÑÑ‚Ð¾Ð³Ð¾ Ð½Ð°Ð¼ Ð½Ðµ Ð¾Ð±Ð¾Ð¹Ñ‚Ð¸ÑÑŒ Ð±ÐµÐ· ÑÑ‚Ð¾Ð¹ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð² Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ… Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ‚ÐµÐºÑÑ‚Ð° Ñ‚Ð°ÐºÐ¸Ñ… ÐºÐ°Ðº Ð¼Ð°ÑˆÐ¸Ð½Ð½Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´, Ð¿Ð¾Ñ‚Ð¾Ð¼Ñƒ Ñ‡Ñ‚Ð¾ Ð½Ð° Ð²Ñ‹Ñ…Ð¾Ð´Ðµ Ð¸Ð· Ð½Ð°ÑˆÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¼Ñ‹ Ð¾Ð¶Ð¸Ð´Ð°ÐµÐ¼ Ð½Ðµ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ñ‡Ð¸ÑÐµÐ», Ð° Ð¿ÐµÑ€ÐµÐ²ÐµÐ´Ñ‘Ð½Ð½Ñ‹Ð¹ Ð½Ð° Ð´Ñ€ÑƒÐ³Ð¾Ð¹ ÑÐ·Ñ‹Ðº Ñ‚ÐµÐºÑÑ‚."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vI3sByQyeNPs",
        "outputId": "7a13bf83-f35f-43d8-ef45-fc9713879ee1"
      },
      "source": [
        "tokenizer.decode(e.ids)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is a text with UNK'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQAq6d9dfqrr"
      },
      "source": [
        "Ð’Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ñ‚Ð°ÐºÐ¶Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð°Ñ‚Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚ `.tokens` Ñƒ Ð¾Ð±ÑŠÐµÐºÑ‚Ð° `Encoding` Ð¸ Ð¾Ð½ Ð²Ñ‹Ð´Ð°ÑÑ‚ Ð²Ð°Ð¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð² ÑÑ‚Ð¾Ð¼ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¸. ÐžÐ±Ñ€Ð°Ñ‚Ð¸Ñ‚Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ, Ñ‡Ñ‚Ð¾ Ð² Ð½Ñ‘Ð¼ Ñ…Ñ€Ð°Ð½ÑÑ‚ÑÑ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ñ‚ÐµÐºÑÑ‚Ð°, ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÐµÐ³Ð¾ Ð½Ðµ ÑÑ‚Ð¾Ð¸Ñ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð»Ñ Ð´ÐµÐ±Ð°Ð³Ð°."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS1sTaocfUQj",
        "outputId": "25781e9e-59a1-4359-a532-b6911a9f656e"
      },
      "source": [
        "e.tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'a', 'text', 'with', 'UNK']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2lOuyQGgHde"
      },
      "source": [
        "Ð•Ñ‰Ñ‘ Ð¾Ð´Ð½Ð° Ð²Ð°Ð¶Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ `Tokenizer` - ÑÑ‚Ð¾ Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð²Ð°Ð¼ id Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÑÐ»Ð¾Ð²Ð° Ð¸Ð»Ð¸ Ð½Ð°Ð¾Ð±Ð¾Ñ€Ð¾Ñ‚ - Ð²Ñ‹Ð´Ð°Ð²Ð°Ñ‚ÑŒ ÑÐ»Ð¾Ð²Ð¾ Ð¿Ð¾ ÐµÐ³Ð¾ id. Ð­Ñ‚Ð¸ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð´ÐµÐ»Ð°ÑŽÑ‚ÑÑ Ð¼ÐµÑ‚Ð¾Ð´Ð°Ð¼Ð¸ `token_to_id` Ð¸ `id_to_token`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC-_wQZ-frw2",
        "outputId": "f9adb0e2-3646-4b4c-d6d4-ebcb69f82509"
      },
      "source": [
        "tokenizer.token_to_id('this')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xFKByRI7gO3u",
        "outputId": "7b89d370-b500-4e2c-c4a7-e370fac4e874"
      },
      "source": [
        "tokenizer.id_to_token(8)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMhM0M-agZ1P"
      },
      "source": [
        "# Saving and loading the tokenizer\n",
        "\n",
        "Ð•ÑÐ»Ð¸ Ð²Ñ‹ Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ Ð·Ð°Ð´ÐµÐ¿Ð»Ð¾Ð¸Ñ‚ÑŒ Ð²Ð°ÑˆÑƒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, Ð²Ð°Ð¼ Ð½ÑƒÐ¶Ð½Ñ‹ Ð´Ð²Ðµ Ð²ÐµÑ‰Ð¸: Ð²Ð°ÑˆÐ° Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³Ð°. `Tokenizer` Ð¿Ð¾ ÑÑƒÑ‚Ð¸ Ð¸ ÑÐ²Ð»ÑÐµÑ‚ÑÑ ÑÑ‚Ð¸Ð¼ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð¼ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³Ð°, Ñ‚Ðº Ð¾Ð½ Ð¼Ð¾Ð¶ÐµÑ‚ Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ‚ÑŒ Ð² ÑÐµÐ±Ñ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑˆÐ°Ð³Ð¾Ð²: Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ñ†Ð¸Ñ Ð¸ Ð¿Ñ€ÐµÑ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ñ, Ñ€Ð°Ð·Ð±Ð¸Ð²Ð°Ð½Ð¸Ðµ Ñ‚ÐµÐºÑÑ‚Ð° Ð½Ð° Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ð¸ Ð½ÑƒÐ¼ÐµÑ€Ð¸ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° `Tokenizer` Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð° Ð¾Ñ‡ÐµÐ½ÑŒ Ð¿Ñ€Ð¾ÑÑ‚Ð¾."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUIQQsgfgVam"
      },
      "source": [
        "tokenizer.save('tokenizer.json')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj-V1eKfhVeG"
      },
      "source": [
        "Ð”Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ð¼ ÐºÐ°Ðº Ð²Ñ‹Ð³Ð»ÑÐ´Ð¸Ñ‚ ÑÑ‚Ð¾Ñ‚ Ñ„Ð°Ð¹Ð»"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIUmGIUdhPo8",
        "outputId": "ea9cb99e-83bd-42e9-87ed-9ebeed4be086"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('tokenizer.json') as f:\n",
        "    tokenizer_json = json.load(f)\n",
        "\n",
        "tokenizer_json"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'version': '1.0',\n",
              " 'truncation': None,\n",
              " 'padding': None,\n",
              " 'added_tokens': [],\n",
              " 'normalizer': None,\n",
              " 'pre_tokenizer': {'type': 'Whitespace'},\n",
              " 'post_processor': None,\n",
              " 'decoder': None,\n",
              " 'model': {'type': 'WordLevel',\n",
              "  'vocab': {'UNK': 0,\n",
              "   'a': 1,\n",
              "   'list': 2,\n",
              "   'of': 3,\n",
              "   'sentences': 4,\n",
              "   'from': 5,\n",
              "   'my': 6,\n",
              "   'dataset': 7,\n",
              "   'this': 8,\n",
              "   'is': 9,\n",
              "   'text': 10,\n",
              "   'with': 11,\n",
              "   'known': 12,\n",
              "   'words': 13},\n",
              "  'unk_token': 'UNK'}}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xarM46NohZUs"
      },
      "source": [
        "ÐœÑ‹ Ð²Ð¸Ð´Ð¸Ð¼, Ñ‡Ñ‚Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ ÑÐµÑ€Ð¸Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ñ‚ ÐºÐ°Ðº Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¸ Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼Ñ‹Ð¹ json Ð¸ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð² ÑÐµÐ±Ðµ Ð¿Ñ€ÐµÑ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ `Whitespace`, ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ, ÑƒÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð½Ð° Ñ‚Ð¾, ÐºÐ°ÐºÐ¾Ð¹ Ñ‚Ð¾ÐºÐµÐ½ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÐºÐ°Ðº UNK-Ñ‚Ð¾ÐºÐµÐ½ Ð¸ Ð¼Ð½Ð¾Ð³Ð¾Ðµ Ð´Ñ€ÑƒÐ³Ð¾Ðµ Ð¾ Ñ‡Ñ‘Ð¼ Ð¼Ñ‹ Ñ Ð²Ð°Ð¼Ð¸ Ð¿Ð¾Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ð¼ Ð² Ð±ÑƒÐ´ÑƒÑ‰Ð¸Ñ… Ð·Ð°Ð½ÑÑ‚Ð¸ÑÑ….\n",
        "\n",
        "Ð”Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¼ÐµÑ‚Ð¾Ð´ `from_file`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmLZD23XhUqS",
        "outputId": "e71a43b9-517e-42e1-f945-4417c74a7ab7"
      },
      "source": [
        "loaded_tokenizer = tokenizers.Tokenizer.from_file('tokenizer.json')\n",
        "e = loaded_tokenizer.encode('this is a text')\n",
        "e"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=4, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkG6ppi6iBfU",
        "outputId": "82c53491-314f-4d67-b8fb-0291f10e8130"
      },
      "source": [
        "e.ids"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 9, 1, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4eRWHJZiNZJ"
      },
      "source": [
        "# Batching\n",
        "\n",
        "Ð•Ñ‰Ñ‘ Ð¾Ð´Ð½Ð° Ð²Ð°Ð¶Ð½Ð°Ñ Ñ„Ð¸Ñ‡Ð° ðŸ¤— Tokenizers - ÑÑ‚Ð¾ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ ÑÐ½ÐºÐ¾Ð´Ð¸Ñ‚ÑŒ Ð¸ Ð´ÐµÐºÐ¾Ð´Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑÑ‚Ñ‹ Ð² Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ñ‚Ñ€ÑÐ´Ð°Ñ…. Ð­Ñ‚Ð¾ ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑÑ Ð¾Ñ‡ÐµÐ½ÑŒ Ð²Ð°Ð¶Ð½Ð¾, Ñ‚Ðº CPU-bottlenecks (ÐºÐ¾Ð³Ð´Ð° Ð¿Ñ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð½Ð¸Ð¼Ð°ÐµÑ‚ ÑÐ¾Ð¿Ð¾ÑÑ‚Ð°Ð²Ð¸Ð¼Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ Ñ forward pass Ð²Ð°ÑˆÐµÐ¹ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚Ð¸) Ð¾Ñ‡ÐµÐ½ÑŒ Ñ‡Ð°ÑÑ‚Ñ‹ Ð² NLP Ð¸ Ð¼Ð¾Ð³ÑƒÑ‚ ÑÐ¸Ð»ÑŒÐ½Ð¾ Ð·Ð°Ð¼ÐµÐ´Ð»Ð¸Ñ‚ÑŒ Ð²Ð°ÑˆÑƒ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÑƒ, Ñ‚Ðº GPU Ð±ÑƒÐ´ÐµÑ‚ Ð¶Ð´Ð°Ñ‚ÑŒ Ð¿Ñ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¸ Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð´ÐµÐ»Ð°Ñ‚ÑŒ Ð² ÑÑ‚Ð¾ Ð²Ñ€ÐµÐ¼Ñ.\n",
        "\n",
        "Ð”Ð»Ñ Ñ‚Ð¾Ð³Ð¾, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ ÑÐ½ÐºÐ¾Ð´Ð¸Ð½Ð³ Ð² Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒ, Ð²Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¼ÐµÑ‚Ð¾Ð´ `encode_batch`. Ð­Ñ‚Ð° Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑÑ Ð¾Ñ‡ÐµÐ½ÑŒ Ð²Ð°Ð¶Ð½Ð¾Ð¹, ÐºÐ¾Ð³Ð´Ð° Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð±Ð°Ñ‚Ñ‡Ð° Ð±Ð»Ð¸Ð·Ð¾Ðº Ðº 64 Ð¸Ð»Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐµ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4So4EDTiJKl",
        "outputId": "3e0f28c6-6d47-49c9-a1ca-cc6c837eca2d"
      },
      "source": [
        "texts"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a list of sentences from my dataset', 'this is a text with known words']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLhnX-EGiMg2",
        "outputId": "7cff4f2a-9e6a-4505-da51-3ae0bae2d11d"
      },
      "source": [
        "batch = tokenizer.encode_batch(texts)\n",
        "batch"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
              " Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkV-S6VOjeU8",
        "outputId": "6add3a2e-97b9-4d91-e240-e3b393796e4b"
      },
      "source": [
        "batch[0].ids, batch[1].ids"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 2, 3, 4, 5, 6, 7], [8, 9, 1, 10, 11, 12, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9VM_KlsnciA"
      },
      "source": [
        "Ð•ÑÐ»Ð¸ Ð²Ñ‹ Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð¿Ð¾Ð·Ð½Ð°ÐºÐ¾Ð¼Ð¸Ñ‚ÑŒÑÑ Ñ ðŸ¤— Tokenizers, Ð¼Ñ‹ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼ Ð²Ð°Ð¼ Ð¿Ð¾Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ð¸Ñ… [Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÑŽ](https://huggingface.co/docs/tokenizers/python/latest). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkUWn-rUmnli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5a7d82-fc66-47e7-e010-8a67920a2e46"
      },
      "source": [
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "!unzip wikitext-103-raw-v1.zip"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-23 19:05:11--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.81.27, 54.231.203.96, 52.217.34.62, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.81.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: â€˜wikitext-103-raw-v1.zipâ€™\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  61.4MB/s    in 3.0s    \n",
            "\n",
            "2023-01-23 19:05:14 (61.4 MB/s) - â€˜wikitext-103-raw-v1.zipâ€™ saved [191984949/191984949]\n",
            "\n",
            "Archive:  wikitext-103-raw-v1.zip\n",
            "   creating: wikitext-103-raw/\n",
            "  inflating: wikitext-103-raw/wiki.test.raw  \n",
            "  inflating: wikitext-103-raw/wiki.valid.raw  \n",
            "  inflating: wikitext-103-raw/wiki.train.raw  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE"
      ],
      "metadata": {
        "id": "WNTmHPYFyiMM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))"
      ],
      "metadata": {
        "id": "bjCWG8fUyu-I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train our tokenizer on the wikitext files, we will need to instantiate a [trainer]{.title-ref}, in this case a BpeTrainer"
      ],
      "metadata": {
        "id": "QRQtqyfJy-tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
      ],
      "metadata": {
        "id": "mpHDQRWly-WN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can set the training arguments like vocab_size or min_frequency (here left at their default values of 30,000 and 0) but the most important part is to give the special_tokens we plan to use later on (they are not used at all during training) so that they get inserted in the vocabulary.\n",
        "\n",
        "The order in which you write the special tokens list matters: here \"[UNK]\" will get the ID 0, \"[CLS]\" will get the ID 1 and so forth.\n",
        "\n",
        "We could train our tokenizer right now, but it wouldnâ€™t be optimal. Without a pre-tokenizer that will split our inputs into words, we might get tokens that overlap several words: for instance we could get an \"it is\" token since those two words often appear next to each other. Using a pre-tokenizer will ensure no token is bigger than a word returned by the pre-tokenizer. Here we want to train a subword BPE tokenizer, and we will use the easiest pre-tokenizer possible by splitting on whitespace."
      ],
      "metadata": {
        "id": "0n_DQtWe0X1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "metadata": {
        "id": "DtOTdxaXy2wH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [f'/content/wikitext-103-raw/wiki.{split}.raw' for split in ['test', 'train', 'valid']]\n",
        "tokenizer.train(files, trainer)"
      ],
      "metadata": {
        "id": "D2YrLn1E0jjw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " To save the tokenizer in one file that contains all its configuration and vocabulary, just use the Tokenizer.save method:"
      ],
      "metadata": {
        "id": "f-OvIlsT1miN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save('/content/tokenizer-wiki.json')"
      ],
      "metadata": {
        "id": "YAJDblqg1JtH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and you can reload your tokenizer from that file with the Tokenizer.from_file classmethod:"
      ],
      "metadata": {
        "id": "ctbTiiq310VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file('/content/tokenizer-wiki.json')"
      ],
      "metadata": {
        "id": "PuonCc2X1yDq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the tokenizer\n",
        "Now that we have trained a tokenizer, we can use it on any text we want with the Tokenizer.encode method:"
      ],
      "metadata": {
        "id": "tO7FatAc3mXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"Hello, y'all! How are you ðŸ˜ ?\")"
      ],
      "metadata": {
        "id": "BJPyPI-D1-5a"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This applied the full pipeline of the tokenizer on the text, returning an Encoding object.\n",
        "\n",
        "This Encoding object then has all the attributes you need for your deep learning model (or other). The tokens attribute contains the segmentation of your text in tokens:"
      ],
      "metadata": {
        "id": "ES70SJ0h3ypy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrAhx1qM3ukr",
        "outputId": "e0a6fe12-7d5d-48fe-8d45-b5e0141dc11d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, the ids attribute will contain the index of each of those tokens in the tokenizerâ€™s vocabulary:"
      ],
      "metadata": {
        "id": "B3SKA5Th4Kx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjlULLZT4H0t",
        "outputId": "f41d0857-a1f4-4f59-85c0-6005f51b657f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important feature of the ðŸ¤— Tokenizers library is that it comes with full alignment tracking, meaning you can always get the part of your original sentence that corresponds to a given token. Those are stored in the offsets attribute of our Encoding object. For instance, letâ€™s assume we would want to find back what caused the \"[UNK]\" token to appear, which is the token at index 9 in the list, we can just ask for the offset at the index:"
      ],
      "metadata": {
        "id": "RIEIqK-X4PpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.offsets[9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E08CBdSM4ORp",
        "outputId": "49568a69-265a-48e4-aba4-d5eb8b462e2d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(26, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and those are the indices that correspond to the emoji in the original sentence:"
      ],
      "metadata": {
        "id": "_OncIuTk7M6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Hello, y'all! How are you ðŸ˜ ?\"\n",
        "sentence[26:27]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OXSuT1MX4ecy",
        "outputId": "9ab00c45-8c44-4615-d644-c59cffb511d8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ðŸ˜'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-processing\n",
        "\n",
        "We might want our tokenizer to automatically add special tokens, like \"[CLS]\" or \"[SEP]\". To do this, we use a post-processor. TemplateProcessing is the most commonly used, you just have to specify a template for the processing of single sentences and pairs of sentences, along with the special tokens and their IDs.\n",
        "\n",
        "When we built our tokenizer, we set \"[CLS]\" and \"[SEP]\" in positions 1 and 2 of our list of special tokens, so this should be their IDs. To double-check, we can use the Tokenizer.token_to_id method:"
      ],
      "metadata": {
        "id": "cnYkHOu87nCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.token_to_id('[SEP]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo0w_UTG7U6z",
        "outputId": "97bed26d-0ccf-4589-8c2c-1105019048c7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how we can set the post-processing to give us the traditional BERT inputs:"
      ],
      "metadata": {
        "id": "xO3UWPVK79wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single='[CLS] $A [SEP]',\n",
        "    pair='[CLS] $A [SEP] $B:1 [SEP]:1',\n",
        "    special_tokens=[\n",
        "        ('[CLS]', tokenizer.token_to_id('[CLS]')),\n",
        "        ('[SEP]', tokenizer.token_to_id('[SEP]'))\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "MwnHClHS7uHC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s go over this snippet of code in more details. First we specify the template for single sentences: those should have the form \"[CLS] \\$A [SEP]\" where \\$A represents our sentence.\n",
        "\n",
        "Then, we specify the template for sentence pairs, which should have the form \"[CLS] \\$A [SEP] \\$B [SEP]\" where \\$A represents the first sentence and $B the second one. The :1 added in the template represent the type IDs we want for each part of our input: it defaults to 0 for everything (which is why we donâ€™t have \\$A:0) and here we set it to 1 for the tokens of the second sentence and the last \"[SEP]\" token.\n",
        "\n",
        "Lastly, we specify the special tokens we used and their IDs in our tokenizerâ€™s vocabulary.\n",
        "\n",
        "To check out this worked properly, letâ€™s try to encode the same sentence as before:"
      ],
      "metadata": {
        "id": "PL8VD0Em9Ck-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"Hello, y'all! How are you ðŸ˜ ?\")\n",
        "print(output.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYoEBr5z8uv2",
        "outputId": "770b6eec-1ef7-4e4f-93bd-d424c668df43"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the results on a pair of sentences, we just pass the two sentences to Tokenizer.encode:"
      ],
      "metadata": {
        "id": "R9MLn5E7-Qn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"Hello, y'all!\", \"How are you ðŸ˜ ?\")\n",
        "print(output.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM9NNlLO-LBH",
        "outputId": "27279ad5-b259-42e6-e376-049ed9300d2b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'How', 'are', 'you', '[UNK]', '?', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then check the type IDs attributed to each token is correct with"
      ],
      "metadata": {
        "id": "fzTHOmjE-aKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.type_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9acxfYXQ-S_J",
        "outputId": "d2694fe6-00f6-4d74-aa83-6f06672d2687"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you save your tokenizer with Tokenizer.save, the post-processor will be saved along.\n",
        "\n",
        "Encoding multiple sentences in a batch\n",
        "To get the full speed of the ðŸ¤— Tokenizers library, itâ€™s best to process your texts by batches by using the Tokenizer.encode_batch method:"
      ],
      "metadata": {
        "id": "TrahFMgE-pSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you ðŸ˜ ?\"])"
      ],
      "metadata": {
        "id": "I2_OT60n-c_S"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is then a list of Encoding objects like the ones we saw before. You can process together as many texts as you like, as long as it fits in memory.\n",
        "\n",
        "To process a batch of sentences pairs, pass two lists to the Tokenizer.encode_batch method: the list of sentences A and the list of sentences B:"
      ],
      "metadata": {
        "id": "xF9VFKYR-8IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode_batch(\n",
        "    [[\"Hello, y'all!\", \"How are you ðŸ˜ ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n",
        ")"
      ],
      "metadata": {
        "id": "TmEJNDT0-1Ud"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When encoding multiple sentences, you can automatically pad the outputs to the longest sentence present by using Tokenizer.enable_padding, with the pad_token and its ID (which we can double-check the id for the padding token with Tokenizer.token_to_id like before):"
      ],
      "metadata": {
        "id": "N5vRFhJg_ViE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.enable_padding(pad_id=3, pad_token='[PAD]')"
      ],
      "metadata": {
        "id": "m5buniJE_GwP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can set the direction of the padding (defaults to the right) or a given length if we want to pad every sample to that specific number (here we leave it unset to pad to the size of the longest text)."
      ],
      "metadata": {
        "id": "y3XDWpul_xYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you ðŸ˜ ?\"])\n",
        "print(output[1].tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VponTvPT_h__",
        "outputId": "e138656e-04a2-417b-b994-c1987a9a90cd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'How', 'are', 'you', '[UNK]', '?', '[SEP]', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the attention mask generated by the tokenizer takes the padding into account:"
      ],
      "metadata": {
        "id": "LHRaltr_AD-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[1].attention_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkepkBWc__Uo",
        "outputId": "cbebccdd-b251-489d-870f-8417c7abc9bd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained\n",
        "\n",
        "Using a pretrained tokenizer\n",
        "\n",
        "You can load any tokenizer from the Hugging Face Hub as long as a tokenizer.json file is available in the repository."
      ],
      "metadata": {
        "id": "Qfn6Q63sARUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "pwW5RUHeANgo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing a pretrained tokenizer from legacy vocabulary files\n",
        "You can also import a pretrained tokenizer directly in, as long as you have its vocabulary file. For instance, here is how to import the classic pretrained BERT tokenizer:"
      ],
      "metadata": {
        "id": "ieXe_ttRBqge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY07WmbxCK6Q",
        "outputId": "1556ce85-09cd-4bd6-c9da-c49183f5b883"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-23 19:09:25--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.41.134, 54.231.169.72, 52.217.164.64, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.41.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231508 (226K) [text/plain]\n",
            "Saving to: â€˜bert-base-uncased-vocab.txtâ€™\n",
            "\n",
            "\r          bert-base   0%[                    ]       0  --.-KB/s               \rbert-base-uncased-v 100%[===================>] 226.08K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2023-01-23 19:09:25 (23.4 MB/s) - â€˜bert-base-uncased-vocab.txtâ€™ saved [231508/231508]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer('bert-base-uncased-vocab.txt', lowercase=True)"
      ],
      "metadata": {
        "id": "3JTX-1odAjuK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "14hXbPj2CBB1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}