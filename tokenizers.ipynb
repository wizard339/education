{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wizard339/education/blob/main/tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI_L2TBSZadk"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s09CQfFyURY8"
      },
      "source": [
        "# ü§ó Tokenizers\n",
        "\n",
        "–î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º –Ω–∞–º –Ω—É–∂–Ω–æ —É–º–µ—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å –µ–≥–æ –≤ –ø–æ–Ω—è—Ç–Ω–æ–º –∫–æ–ø–º—å—é—Ç–µ—Ä—É –≤–∏–¥–µ –∏ –ø–µ—Ä–≤—ã–µ —à–∞–≥–∏ –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ - —ç—Ç–æ —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã (–∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–ª–æ–≤–∞–º–∏, –∞ –º–æ–≥—É—Ç –∏ –Ω–µ –±—ã—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏) –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä –æ—Ç 0 –¥–æ —Ä–∞–∑–º–µ—Ä–∞ –≤–∞—à–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è. –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –º—ã —Å–º–æ–∂–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫—É —Ç–∏–ø–∞ \"—ç—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞\" –≤ –≤–µ–∫—Ç–æ—Ä –≤–∏–¥–∞ `[47, 392, 38]`. –≠—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏—è - –∑–∞–º–µ–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã - —á–∞—Å—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è **–Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏—è**, –∞ –æ–±—Ä–∞—Ç–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è - **–¥–µ–Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏—è**.\n",
        "\n",
        "–ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –º–æ–¥—É–ª—å `sklearn.feature_extraction.text`, –Ω–∞–ø—Ä–∏–º–µ—Ä –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é `TfidfVectorizer`, sklearn –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ —ç—Ç–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∑–∞ –≤–∞—Å –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º, –Ω–æ –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏, –∂–∏–∑–Ω—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≥–æ—Ä–∞–∑–¥–æ —Å–ª–æ–∂–Ω–µ–µ –∏ –¥–æ –Ω–µ–¥–∞–≤–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—á—Ç–∏ –≤—Å–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Å–∞–º–æ–ø–∏—Å–Ω—ã–µ —à—Ç—É–∫–∏, —á—Ç–æ–±—ã (–¥–µ)–Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã. –≠—Ç–æ —Å–æ–∑–¥–∞–≤–∞–ª–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –Ω–µ—É–¥–æ–±—Å—Ç —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Ç–µ–º, —á—Ç–æ –≤–æ-–ø–µ—Ä–≤—ã—Ö –≤–∞–º –ø—Ä–∏—Ö–æ–¥–∏–ª–æ—Å—å –ø–∏—Å–∞—Ç—å –±–æ–ª—å—à–µ –∫–æ–¥–∞ (–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –±–æ–ª—å—à–µ –æ—à–∏–±–æ–∫), –≤–æ-–≤—Ç–æ—Ä—ã—Ö —Ä—è–¥–æ–º —Å –Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏–µ–π –ª–µ–∂–∏—Ç –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –º–µ–ª–∫–∏—Ö –ø–æ–¥–∑–∞–¥–∞—á –≤–∫–ª—é—á–∞—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ UNK, —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞—à–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –∏ —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è. \n",
        "\n",
        "Huggingface Tokenizers –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤—Å—ë –±–æ–ª–µ–µ –∏ –±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ –≤ NLP-—Å–æ–æ–±—â–µ—Å—Ç–≤–µ. –≠—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –±—ã—Å—Ç—Ä–æ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ - –æ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ —Å–ª–æ–≤–∞ –ø–æ –ø—Ä–æ–±–µ–ª—É –¥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ –±–∏—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è unicode-—Å—Ç—Ä–æ–∫ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —á–∞—Å—Ç–æ—Ç—ã –±–∏—Ç–æ–≤–æ–π n-gram. –¢–∞–∫–∂–µ –æ–Ω–∞ —É–º–µ–µ—Ç –¥–µ–ª–∞—Ç—å –Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏—é –∏ –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ —Ä–∞–±–æ—Ç–∞–µ—Ç. –í–º–µ—Å—Ç–æ —Å–∞–º–æ–ø–∏—Å–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—ë –µ—â—ë —á–∞—Å—Ç–æ –ø—Ä–µ–ø–æ–¥–∞—é—Ç—Å—è –≤ –∫—É—Ä—Å–∞—Ö –ø–æ NLP, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ü§ó Tokenizers –≤ —Ç–µ—á–µ–Ω–∏–µ –≤—Å–µ–≥–æ –∫—É—Ä—Å–∞ –∏ –¥–æ–≤–æ–ª—å–Ω–æ –±—ã—Å—Ç—Ä–æ –≤—ã –Ω–∞—É—á–∏—Ç–µ—Å—å –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–∞–∂–µ –Ω–µ —Å–∞–º—ã–º–∏ –æ—á–µ–≤–∏–¥–Ω—ã–º–∏ –µ—ë –º–µ—Ç–æ–¥–∞–º–∏. –ê —Å–µ–π—á–∞—Å –º—ã –Ω–∞—á–Ω—ë–º —Å —Å–∞–º—ã—Ö –æ—Å–Ω–æ–≤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DLYms6MBUPYm",
        "outputId": "e3f433ce-fadd-44c1-f415-36e6a6703a02"
      },
      "source": [
        "import datasets\n",
        "import tokenizers\n",
        "tokenizers.__version__  # should be above or equal 0.10.0rc1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.13.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww0-lHJwaSMr"
      },
      "source": [
        "–ó–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞, —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –µ–≥–æ –Ω–∞ —Ç–æ–∫–µ–Ω—ã –∏ –Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ ü§ó Tokenizers –æ—Ç–≤–µ—á–∞–µ—Ç –æ–±—ä–µ–∫—Ç `Tokenizer`. –°–∞–º—ã–π –Ω–∞–≥–ª—è–¥–Ω—ã–π —Å–ø–æ—Å–æ–± –µ–≥–æ —Å–æ–∑–¥–∞—Ç—å - —ç—Ç–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –µ–º—É —Å–ª–æ–≤–∞—Ä—å, –º–∞–ø—è—â–∏–π —Å–ª–æ–≤–∞ –Ω–∞ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã. –î–∞–≤–∞–π—Ç–µ –Ω–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —ç—Ç–æ –¥–µ–ª–∞–µ—Ç.\n",
        "\n",
        "**NOTE:** –°–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ - —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –Ω–∞ —Å–ª–æ–≤–∞ —Å –ø–æ–º–æ—â—å—é regex - —Å—Ç–æ–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–±–æ–∫—É –≤ Tokenizers –∏ –¥–∞–∂–µ –Ω–µ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥–æ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, —Ç–∫ –∑–∞—á–∞—Å—Ç—É—é –æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –ø–µ—Ä–≤—ã–π —à–∞–≥ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –ü–æ—ç—Ç–æ–º—É –æ–Ω–∞ –∑–∞–¥–∞—ë—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–±—ä–µ–∫—Ç–∞ –≤–∏–¥–∞ `pre_tokenizer`.\n",
        "\n",
        "–î–ª—è —ç—Ç–æ–≥–æ –º—ã –¥–µ–ª–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤ –Ω–∞—á–∞–ª–µ —Å–æ–∑–¥–∞—ë—Ç –æ–±—ä–µ–∫—Ç `pre_tokenizer` –∏ –ø—Ä–µ–ø–æ—Ü–µ—Å—Å–∏—Ç –Ω–∞—à–∏ —Ç–µ–∫—Å—Ç—ã. –ü–æ—Å–ª–µ —á–µ–≥–æ –º—ã –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã –Ω–∞—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –≤—ã–±–∏—Ä–∞–µ—Ç top-N —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö, —ç—Ç–æ —É–¥–æ–±–Ω–æ –¥–µ–ª–∞—Ç—å —Å –ø–æ–º–æ—â—å—é –æ–±—ä–µ–∫—Ç–∞ `Counter` –∏–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ –≤ Python –º–æ–¥—É–ª—è collections. –í –∏–≥—Ä—É—à–µ—á–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ –≤ —Å–ª–µ—â—É—é—â–µ–π —è—á–µ–π–∫–µ, —Å–æ—Å—Ç–æ—è—â–∏–º —Ç–æ–ª—å–∫–æ –ª–∏—à—å –∏–∑ –¥–≤—É—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —ç—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏—è –Ω–µ–≤–∞–∂–Ω–∞, –Ω–æ –∫–æ–≥–¥–∞ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ —Å –í–∏–∫–∏–ø–µ–¥–∏—é, –≤–∞—à —Å–ª–æ–≤–∞—Ä—å –±—ã—Å—Ç—Ä–æ —Å—Ç–∞–Ω–µ—Ç –Ω–µ–æ–±—ä—ë–º–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ –µ–≥–æ –Ω—É–∂–Ω–æ —É–º–µ—Ç—å –æ–≥—Ä–Ω–∏—á–∏–≤–∞—Ç—å.\n",
        "\n",
        "–¢—Ä–µ—Ç–∏–π —à–∞–≥ - —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–∏–Ω–≥–∞ —Å–ª–æ–≤ –Ω–∞ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã. –û–Ω –¥–æ–≤–æ–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ–π –∏ –µ–≥–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ—á–∫—É —Å –ø–æ–º–æ—â—å—é dictionary comprehension. –ò –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –º—ã –≥–æ—Ç–æ–≤—ã —Å–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç `Tokenizer` –∏ –ø—Ä–∏—Å–≤–æ–∏—Ç—å –µ–º—É –ø—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPZ9Exn3ZeKa"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "texts = ['a list of sentences from my dataset', 'this is a text with known words']\n",
        "\n",
        "\n",
        "def make_whitespace_tokenizer(texts, max_vocab_size=10_000, unk_token='UNK'):\n",
        "    pre_tokenizer = Whitespace()\n",
        "    tokenized_texts = [[w for w, _ in pre_tokenizer.pre_tokenize_str(t)] for t in texts]\n",
        "\n",
        "    c = Counter()\n",
        "    for text in tokenized_texts:\n",
        "        c.update(text)\n",
        "\n",
        "    token2id = {word: i + 1 for i, (word, count) in enumerate(c.most_common(max_vocab_size))}\n",
        "\n",
        "    # usually, UNK is assigned index 0 or 1\n",
        "    token2id[unk_token] = 0\n",
        "\n",
        "    tokenizer = tokenizers.Tokenizer(WordLevel(token2id, unk_token))\n",
        "    tokenizer.pre_tokenizer = pre_tokenizer\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "tokenizer = make_whitespace_tokenizer(texts)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpbn6P6Qd2Jq"
      },
      "source": [
        "# Encoding and decoding text\n",
        "\n",
        "–î–ª—è –Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ `.encode()`, –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∏–π –æ–±—ä–µ–∫—Ç –∫–ª–∞—Å—Å–∞ `Encoding`. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –≤ —Å–µ–±–µ –º–Ω–æ–≥–æ –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –Ω–æ —Å–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ –∏–∑ –Ω–µ–≥–æ - `.ids` –¥–∞—é—â–∏–π –Ω–∞–º –Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –≥–¥–µ —Å–ª–æ–≤–∞ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rF_Ouq4bg8k",
        "outputId": "159b3e3f-6447-473a-94b5-f78fa0e6aa90"
      },
      "source": [
        "_text = 'this is a text with unknown_word'\n",
        "e = tokenizer.encode(_text)\n",
        "e"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM1wIMLIiFD_",
        "outputId": "270aebac-fe11-41c7-adc0-0060e5344b3f"
      },
      "source": [
        "e.ids"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 9, 1, 10, 11, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEb3MVLueR8V"
      },
      "source": [
        "–î–µ–Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ–ª–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥–æ–º `.decode()` (–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –Ω–∞ –≤—Ö–æ–¥ –∫ –Ω–µ–º—É –ø—Ä–∏—Ö–æ–¥—è—Ç `.ids`, –∞ –Ω–µ –æ–±—ä–µ–∫—Ç `Encoding`). –≠—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏—è –æ—á–µ–Ω—å –≤–∞–∂–Ω–∞ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –ø–æ –º–Ω–æ–≥–∏–º –ø—Ä–∏—á–∏–Ω–∞–º –∏ –æ–¥–Ω–∞ –∏–∑ –Ω–∏—Ö - –≤—ã –º–æ–∂–µ—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —Ç–µ–∫—Å—Ç —Ç–∞–∫, –∫–∞–∫ –µ–≥–æ –≤–∏–¥–∏—Ç –≤–∞—à–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å - —Å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–∏ —Å–ª–æ–∞–≤–º–∏ –∑–∞–º–µ–Ω–µ–Ω–Ω—ã–º–∏ –Ω–∞ —Ç–æ–∫–µ–Ω UNK –∏, –≤–æ–∑–º–æ–∂–Ω–æ, —Å –≤–∞–º–∏—à–∏ –æ—à–∏–±–∫–∞–º–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞. –≠—Ç–∏ –æ—à–∏–±–∫–∏ –æ—á–µ–Ω—å —á–∞—Å—Ç—ã –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–µ–Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–∏–ª—å–Ω–æ –ø–æ–º–æ–≥–∞–µ—Ç –≤ –¥–µ–±–∞–≥–µ.\n",
        "\n",
        "–ö—Ä–æ–º–µ —ç—Ç–æ–≥–æ –Ω–∞–º –Ω–µ –æ–±–æ–π—Ç–∏—Å—å –±–µ–∑ —ç—Ç–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Ç–∞–∫–∏—Ö –∫–∞–∫ –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥, –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–∞ –≤—ã—Ö–æ–¥–µ –∏–∑ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –º—ã –æ–∂–∏–¥–∞–µ–º –Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∏—Å–µ–ª, –∞ –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–π –Ω–∞ –¥—Ä—É–≥–æ–π —è–∑—ã–∫ —Ç–µ–∫—Å—Ç."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vI3sByQyeNPs",
        "outputId": "7a13bf83-f35f-43d8-ef45-fc9713879ee1"
      },
      "source": [
        "tokenizer.decode(e.ids)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is a text with UNK'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQAq6d9dfqrr"
      },
      "source": [
        "–í—ã –º–æ–∂–µ—Ç–µ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞—Ç—Ç—Ä–∏–±—É—Ç `.tokens` —É –æ–±—ä–µ–∫—Ç–∞ `Encoding` –∏ –æ–Ω –≤—ã–¥–∞—Å—Ç –≤–∞–º —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —ç—Ç–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤ –Ω—ë–º —Ö—Ä–∞–Ω—è—Ç—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Ç–µ–∫—Å—Ç–∞, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –µ–≥–æ –Ω–µ —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –¥–µ–±–∞–≥–∞."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS1sTaocfUQj",
        "outputId": "25781e9e-59a1-4359-a532-b6911a9f656e"
      },
      "source": [
        "e.tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'a', 'text', 'with', 'UNK']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2lOuyQGgHde"
      },
      "source": [
        "–ï—â—ë –æ–¥–Ω–∞ –≤–∞–∂–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è `Tokenizer` - —ç—Ç–æ –¥–∞–≤–∞—Ç—å –≤–∞–º id –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –∏–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç - –≤—ã–¥–∞–≤–∞—Ç—å —Å–ª–æ–≤–æ –ø–æ –µ–≥–æ id. –≠—Ç–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–µ–ª–∞—é—Ç—Å—è –º–µ—Ç–æ–¥–∞–º–∏ `token_to_id` –∏ `id_to_token`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC-_wQZ-frw2",
        "outputId": "f9adb0e2-3646-4b4c-d6d4-ebcb69f82509"
      },
      "source": [
        "tokenizer.token_to_id('this')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xFKByRI7gO3u",
        "outputId": "7b89d370-b500-4e2c-c4a7-e370fac4e874"
      },
      "source": [
        "tokenizer.id_to_token(8)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMhM0M-agZ1P"
      },
      "source": [
        "# Saving and loading the tokenizer\n",
        "\n",
        "–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –∑–∞–¥–µ–ø–ª–æ–∏—Ç—å –≤–∞—à—É –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –≤–∞–º –Ω—É–∂–Ω—ã –¥–≤–µ –≤–µ—â–∏: –≤–∞—à–∞ –º–æ–¥–µ–ª—å –∏ –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞. `Tokenizer` –ø–æ —Å—É—Ç–∏ –∏ —è–≤–ª—è–µ—Ç—Å—è —ç—Ç–∏–º –º–µ—Ç–æ–¥–æ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞, —Ç–∫ –æ–Ω –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤: –Ω–æ—Ä–º–∞–ª–∏–∑—Ü–∏—è –∏ –ø—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —Ä–∞–∑–±–∏–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã –∏ –Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏—è. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ `Tokenizer` —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUIQQsgfgVam"
      },
      "source": [
        "tokenizer.save('tokenizer.json')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj-V1eKfhVeG"
      },
      "source": [
        "–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç —ç—Ç–æ—Ç —Ñ–∞–π–ª"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIUmGIUdhPo8",
        "outputId": "ea9cb99e-83bd-42e9-87ed-9ebeed4be086"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('tokenizer.json') as f:\n",
        "    tokenizer_json = json.load(f)\n",
        "\n",
        "tokenizer_json"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'version': '1.0',\n",
              " 'truncation': None,\n",
              " 'padding': None,\n",
              " 'added_tokens': [],\n",
              " 'normalizer': None,\n",
              " 'pre_tokenizer': {'type': 'Whitespace'},\n",
              " 'post_processor': None,\n",
              " 'decoder': None,\n",
              " 'model': {'type': 'WordLevel',\n",
              "  'vocab': {'UNK': 0,\n",
              "   'a': 1,\n",
              "   'list': 2,\n",
              "   'of': 3,\n",
              "   'sentences': 4,\n",
              "   'from': 5,\n",
              "   'my': 6,\n",
              "   'dataset': 7,\n",
              "   'this': 8,\n",
              "   'is': 9,\n",
              "   'text': 10,\n",
              "   'with': 11,\n",
              "   'known': 12,\n",
              "   'words': 13},\n",
              "  'unk_token': 'UNK'}}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xarM46NohZUs"
      },
      "source": [
        "–ú—ã –≤–∏–¥–∏–º, —á—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ–π –∏ —á–∏—Ç–∞–µ–º—ã–π json –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤ —Å–µ–±–µ –ø—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä `Whitespace`, —Å–ª–æ–≤–∞—Ä—å, —É–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–æ, –∫–∞–∫–æ–π —Ç–æ–∫–µ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ UNK-—Ç–æ–∫–µ–Ω –∏ –º–Ω–æ–≥–æ–µ –¥—Ä—É–≥–æ–µ –æ —á—ë–º –º—ã —Å –≤–∞–º–∏ –ø–æ–≥–æ–≤–æ—Ä–∏–º –≤ –±—É–¥—É—â–∏—Ö –∑–∞–Ω—è—Ç–∏—è—Ö.\n",
        "\n",
        "–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ `from_file`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmLZD23XhUqS",
        "outputId": "e71a43b9-517e-42e1-f945-4417c74a7ab7"
      },
      "source": [
        "loaded_tokenizer = tokenizers.Tokenizer.from_file('tokenizer.json')\n",
        "e = loaded_tokenizer.encode('this is a text')\n",
        "e"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=4, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkG6ppi6iBfU",
        "outputId": "82c53491-314f-4d67-b8fb-0291f10e8130"
      },
      "source": [
        "e.ids"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 9, 1, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4eRWHJZiNZJ"
      },
      "source": [
        "# Batching\n",
        "\n",
        "–ï—â—ë –æ–¥–Ω–∞ –≤–∞–∂–Ω–∞—è —Ñ–∏—á–∞ ü§ó Tokenizers - —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —ç–Ω–∫–æ–¥–∏—Ç—å –∏ –¥–µ–∫–æ–¥–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Ç—Ä—ç–¥–∞—Ö. –≠—Ç–æ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ, —Ç–∫ CPU-bottlenecks (–∫–æ–≥–¥–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–Ω–∏–º–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–µ –≤—Ä–µ–º—è —Å forward pass –≤–∞—à–µ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏) –æ—á–µ–Ω—å —á–∞—Å—Ç—ã –≤ NLP –∏ –º–æ–≥—É—Ç —Å–∏–ª—å–Ω–æ –∑–∞–º–µ–¥–ª–∏—Ç—å –≤–∞—à—É —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É, —Ç–∫ GPU –±—É–¥–µ—Ç –∂–¥–∞—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞—Ç—å –≤ —ç—Ç–æ –≤—Ä–µ–º—è.\n",
        "\n",
        "–î–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å —ç–Ω–∫–æ–¥–∏–Ω–≥ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å, –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ `encode_batch`. –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ–π, –∫–æ–≥–¥–∞ —Ä–∞–∑–º–µ—Ä –≤–∞—à–µ–≥–æ –±–∞—Ç—á–∞ –±–ª–∏–∑–æ–∫ –∫ 64 –∏–ª–∏ –±–æ–ª—å—à–µ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4So4EDTiJKl",
        "outputId": "3e0f28c6-6d47-49c9-a1ca-cc6c837eca2d"
      },
      "source": [
        "texts"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a list of sentences from my dataset', 'this is a text with known words']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLhnX-EGiMg2",
        "outputId": "7cff4f2a-9e6a-4505-da51-3ae0bae2d11d"
      },
      "source": [
        "batch = tokenizer.encode_batch(texts)\n",
        "batch"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
              " Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkV-S6VOjeU8",
        "outputId": "6add3a2e-97b9-4d91-e240-e3b393796e4b"
      },
      "source": [
        "batch[0].ids, batch[1].ids"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 2, 3, 4, 5, 6, 7], [8, 9, 1, 10, 11, 12, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9VM_KlsnciA"
      },
      "source": [
        "–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –±–æ–ª—å—à–µ –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å ü§ó Tokenizers, –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –≤–∞–º –ø–æ—á–∏—Ç–∞—Ç—å –∏—Ö [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é](https://huggingface.co/docs/tokenizers/python/latest). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkUWn-rUmnli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5a7d82-fc66-47e7-e010-8a67920a2e46"
      },
      "source": [
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "!unzip wikitext-103-raw-v1.zip"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-23 19:05:11--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.81.27, 54.231.203.96, 52.217.34.62, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.81.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‚Äòwikitext-103-raw-v1.zip‚Äô\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  61.4MB/s    in 3.0s    \n",
            "\n",
            "2023-01-23 19:05:14 (61.4 MB/s) - ‚Äòwikitext-103-raw-v1.zip‚Äô saved [191984949/191984949]\n",
            "\n",
            "Archive:  wikitext-103-raw-v1.zip\n",
            "   creating: wikitext-103-raw/\n",
            "  inflating: wikitext-103-raw/wiki.test.raw  \n",
            "  inflating: wikitext-103-raw/wiki.valid.raw  \n",
            "  inflating: wikitext-103-raw/wiki.train.raw  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE"
      ],
      "metadata": {
        "id": "WNTmHPYFyiMM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))"
      ],
      "metadata": {
        "id": "bjCWG8fUyu-I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train our tokenizer on the wikitext files, we will need to instantiate a [trainer]{.title-ref}, in this case a BpeTrainer"
      ],
      "metadata": {
        "id": "QRQtqyfJy-tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
      ],
      "metadata": {
        "id": "mpHDQRWly-WN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can set the training arguments like vocab_size or min_frequency (here left at their default values of 30,000 and 0) but the most important part is to give the special_tokens we plan to use later on (they are not used at all during training) so that they get inserted in the vocabulary.\n",
        "\n",
        "The order in which you write the special tokens list matters: here \"[UNK]\" will get the ID 0, \"[CLS]\" will get the ID 1 and so forth.\n",
        "\n",
        "We could train our tokenizer right now, but it wouldn‚Äôt be optimal. Without a pre-tokenizer that will split our inputs into words, we might get tokens that overlap several words: for instance we could get an \"it is\" token since those two words often appear next to each other. Using a pre-tokenizer will ensure no token is bigger than a word returned by the pre-tokenizer. Here we want to train a subword BPE tokenizer, and we will use the easiest pre-tokenizer possible by splitting on whitespace."
      ],
      "metadata": {
        "id": "0n_DQtWe0X1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "metadata": {
        "id": "DtOTdxaXy2wH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [f'/content/wikitext-103-raw/wiki.{split}.raw' for split in ['test', 'train', 'valid']]\n",
        "tokenizer.train(files, trainer)"
      ],
      "metadata": {
        "id": "D2YrLn1E0jjw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " To save the tokenizer in one file that contains all its configuration and vocabulary, just use the Tokenizer.save method:"
      ],
      "metadata": {
        "id": "f-OvIlsT1miN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save('/content/tokenizer-wiki.json')"
      ],
      "metadata": {
        "id": "YAJDblqg1JtH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and you can reload your tokenizer from that file with the Tokenizer.from_file classmethod:"
      ],
      "metadata": {
        "id": "ctbTiiq310VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file('/content/tokenizer-wiki.json')"
      ],
      "metadata": {
        "id": "PuonCc2X1yDq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the tokenizer\n",
        "Now that we have trained a tokenizer, we can use it on any text we want with the Tokenizer.encode method:"
      ],
      "metadata": {
        "id": "tO7FatAc3mXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")"
      ],
      "metadata": {
        "id": "BJPyPI-D1-5a"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This applied the full pipeline of the tokenizer on the text, returning an Encoding object.\n",
        "\n",
        "This Encoding object then has all the attributes you need for your deep learning model (or other). The tokens attribute contains the segmentation of your text in tokens:"
      ],
      "metadata": {
        "id": "ES70SJ0h3ypy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrAhx1qM3ukr",
        "outputId": "e0a6fe12-7d5d-48fe-8d45-b5e0141dc11d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, the ids attribute will contain the index of each of those tokens in the tokenizer‚Äôs vocabulary:"
      ],
      "metadata": {
        "id": "B3SKA5Th4Kx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjlULLZT4H0t",
        "outputId": "f41d0857-a1f4-4f59-85c0-6005f51b657f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important feature of the ü§ó Tokenizers library is that it comes with full alignment tracking, meaning you can always get the part of your original sentence that corresponds to a given token. Those are stored in the offsets attribute of our Encoding object. For instance, let‚Äôs assume we would want to find back what caused the \"[UNK]\" token to appear, which is the token at index 9 in the list, we can just ask for the offset at the index:"
      ],
      "metadata": {
        "id": "RIEIqK-X4PpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.offsets[9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E08CBdSM4ORp",
        "outputId": "49568a69-265a-48e4-aba4-d5eb8b462e2d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(26, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and those are the indices that correspond to the emoji in the original sentence:"
      ],
      "metadata": {
        "id": "_OncIuTk7M6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Hello, y'all! How are you üòÅ ?\"\n",
        "sentence[26:27]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OXSuT1MX4ecy",
        "outputId": "9ab00c45-8c44-4615-d644-c59cffb511d8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'üòÅ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-processing\n",
        "\n",
        "We might want our tokenizer to automatically add special tokens, like \"[CLS]\" or \"[SEP]\". To do this, we use a post-processor. TemplateProcessing is the most commonly used, you just have to specify a template for the processing of single sentences and pairs of sentences, along with the special tokens and their IDs.\n",
        "\n",
        "When we built our tokenizer, we set \"[CLS]\" and \"[SEP]\" in positions 1 and 2 of our list of special tokens, so this should be their IDs. To double-check, we can use the Tokenizer.token_to_id method:"
      ],
      "metadata": {
        "id": "cnYkHOu87nCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.token_to_id('[SEP]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo0w_UTG7U6z",
        "outputId": "97bed26d-0ccf-4589-8c2c-1105019048c7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how we can set the post-processing to give us the traditional BERT inputs:"
      ],
      "metadata": {
        "id": "xO3UWPVK79wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single='[CLS] $A [SEP]',\n",
        "    pair='[CLS] $A [SEP] $B:1 [SEP]:1',\n",
        "    special_tokens=[\n",
        "        ('[CLS]', tokenizer.token_to_id('[CLS]')),\n",
        "        ('[SEP]', tokenizer.token_to_id('[SEP]'))\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "MwnHClHS7uHC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs go over this snippet of code in more details. First we specify the template for single sentences: those should have the form \"[CLS] \\$A [SEP]\" where \\$A represents our sentence.\n",
        "\n",
        "Then, we specify the template for sentence pairs, which should have the form \"[CLS] \\$A [SEP] \\$B [SEP]\" where \\$A represents the first sentence and $B the second one. The :1 added in the template represent the type IDs we want for each part of our input: it defaults to 0 for everything (which is why we don‚Äôt have \\$A:0) and here we set it to 1 for the tokens of the second sentence and the last \"[SEP]\" token.\n",
        "\n",
        "Lastly, we specify the special tokens we used and their IDs in our tokenizer‚Äôs vocabulary.\n",
        "\n",
        "To check out this worked properly, let‚Äôs try to encode the same sentence as before:"
      ],
      "metadata": {
        "id": "PL8VD0Em9Ck-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
        "print(output.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYoEBr5z8uv2",
        "outputId": "770b6eec-1ef7-4e4f-93bd-d424c668df43"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the results on a pair of sentences, we just pass the two sentences to Tokenizer.encode:"
      ],
      "metadata": {
        "id": "R9MLn5E7-Qn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"Hello, y'all!\", \"How are you üòÅ ?\")\n",
        "print(output.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM9NNlLO-LBH",
        "outputId": "27279ad5-b259-42e6-e376-049ed9300d2b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'How', 'are', 'you', '[UNK]', '?', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then check the type IDs attributed to each token is correct with"
      ],
      "metadata": {
        "id": "fzTHOmjE-aKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.type_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9acxfYXQ-S_J",
        "outputId": "d2694fe6-00f6-4d74-aa83-6f06672d2687"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you save your tokenizer with Tokenizer.save, the post-processor will be saved along.\n",
        "\n",
        "Encoding multiple sentences in a batch\n",
        "To get the full speed of the ü§ó Tokenizers library, it‚Äôs best to process your texts by batches by using the Tokenizer.encode_batch method:"
      ],
      "metadata": {
        "id": "TrahFMgE-pSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you üòÅ ?\"])"
      ],
      "metadata": {
        "id": "I2_OT60n-c_S"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is then a list of Encoding objects like the ones we saw before. You can process together as many texts as you like, as long as it fits in memory.\n",
        "\n",
        "To process a batch of sentences pairs, pass two lists to the Tokenizer.encode_batch method: the list of sentences A and the list of sentences B:"
      ],
      "metadata": {
        "id": "xF9VFKYR-8IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode_batch(\n",
        "    [[\"Hello, y'all!\", \"How are you üòÅ ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n",
        ")"
      ],
      "metadata": {
        "id": "TmEJNDT0-1Ud"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When encoding multiple sentences, you can automatically pad the outputs to the longest sentence present by using Tokenizer.enable_padding, with the pad_token and its ID (which we can double-check the id for the padding token with Tokenizer.token_to_id like before):"
      ],
      "metadata": {
        "id": "N5vRFhJg_ViE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.enable_padding(pad_id=3, pad_token='[PAD]')"
      ],
      "metadata": {
        "id": "m5buniJE_GwP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can set the direction of the padding (defaults to the right) or a given length if we want to pad every sample to that specific number (here we leave it unset to pad to the size of the longest text)."
      ],
      "metadata": {
        "id": "y3XDWpul_xYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you üòÅ ?\"])\n",
        "print(output[1].tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VponTvPT_h__",
        "outputId": "e138656e-04a2-417b-b994-c1987a9a90cd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'How', 'are', 'you', '[UNK]', '?', '[SEP]', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the attention mask generated by the tokenizer takes the padding into account:"
      ],
      "metadata": {
        "id": "LHRaltr_AD-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[1].attention_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkepkBWc__Uo",
        "outputId": "cbebccdd-b251-489d-870f-8417c7abc9bd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained\n",
        "\n",
        "Using a pretrained tokenizer\n",
        "\n",
        "You can load any tokenizer from the Hugging Face Hub as long as a tokenizer.json file is available in the repository."
      ],
      "metadata": {
        "id": "Qfn6Q63sARUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "pwW5RUHeANgo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing a pretrained tokenizer from legacy vocabulary files\n",
        "You can also import a pretrained tokenizer directly in, as long as you have its vocabulary file. For instance, here is how to import the classic pretrained BERT tokenizer:"
      ],
      "metadata": {
        "id": "ieXe_ttRBqge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY07WmbxCK6Q",
        "outputId": "1556ce85-09cd-4bd6-c9da-c49183f5b883"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-23 19:09:25--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.41.134, 54.231.169.72, 52.217.164.64, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.41.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231508 (226K) [text/plain]\n",
            "Saving to: ‚Äòbert-base-uncased-vocab.txt‚Äô\n",
            "\n",
            "\r          bert-base   0%[                    ]       0  --.-KB/s               \rbert-base-uncased-v 100%[===================>] 226.08K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2023-01-23 19:09:25 (23.4 MB/s) - ‚Äòbert-base-uncased-vocab.txt‚Äô saved [231508/231508]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer('bert-base-uncased-vocab.txt', lowercase=True)"
      ],
      "metadata": {
        "id": "3JTX-1odAjuK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "14hXbPj2CBB1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}